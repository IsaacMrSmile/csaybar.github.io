---
title: "Head Start Data Science I: Titanic Challenge"
subtitle: "TOP 3% Solution 0.823 -- NO CHEATING!"
author: "Cesar Aybar"
date: '2019-05-19'
output:
  html_document:
    toc: true
    toc_float: true
tags: ["Python", "shap", "featuretools", "lightgbm","sklearn"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction

The [Titanic challenge](https://www.kaggle.com/c/titanic/overview/evaluation) is an excellent way to practice the necessary skills required for ML. Like most of us, I first blindly applied a well-known ML method (**Lightgbm**); however,  I couldn't go up over the Top 20% :(.  To have success  in this competition you need to realize an **acute feature engineering** that takes into account the distribution on train and test dataset. This post is the perfect opportunity to share my Python package [preml](https://www.github.com/csaybar/preml) and show you how can you **BEAT THE 97% OF LB**.

Frankly, This is not a 100% ORIGINAL work, I get a lot of inspiration of different kernel:

- [1. Titanic WCG+XGBoost](https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688)
- [2. Automated feature engineering for Titanic dataset](https://www.kaggle.com/liananapalkova/automated-feature-engineering-for-titanic-dataset)
- [3. Exploring Survival on the Titanic](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic)
- [4. Tutorial on hyperopt](https://www.kaggle.com/fanvacoolt/tutorial-on-hyperopt)
- [5. minimal pipeline | lightgbm | + shapley](https://www.kaggle.com/rquintino/minimal-pipeline-lightgbm-shapley)
- [6. My secret sauce to be in top 2% of a kaggle competition](https://towardsdatascience.com/my-secret-sauce-to-be-in-top-2-of-a-kaggle-competition-57cff0677d3c)

So, if you find this post helpful some **UPVOTES** to the previous ones would be very much appreciated.

<center>
<img src=https://cdn-images-1.medium.com/max/1600/1*2T5rbjOBGVFdSvtlhCqlNg.png >
<figcaption>Cross Industry Standard Process for Data Mining (CRISP-DM)</figcaption>
</center>

## 2. Competition Description

The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.
One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.
In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.



## 3. Goal

Predict if a **PASSENGER SURVIVED** the sinking of the Titanic or not. For each in the test set, you must predict a 0 or 1 value for the variable.

The data has been split into two groups:

- training set (train.csv)
- test set (test.csv)

The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.

The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.


Variable Name | Description
--------------|-------------
Survived      | Survived (1) or died (0)
Pclass        | Passenger's class
Name          | Passenger's name
Sex           | Passenger's sex
Age           | Passenger's age
SibSp         | Number of siblings/spouses aboard
Parch         | Number of parents/children aboard
Ticket        | Ticket number
Fare          | Fare
Cabin         | Cabin
Embarked      | Port of embarkation

## 4. Metric

Your score is the percentage of passengers you correctly predict. This is known simply as [**accuracy**](https://en.wikipedia.org/wiki/Accuracy_and_precision#In_binary_classification).

## 5. Specific concepts that will be covered:

In the process, we will build practical experience and develop intuition around the following Python packages:

* **[Feature Tools](https://www.featuretools.com/)** - Feature engineering is fundamental to the application of **machine learning**, and is both difficult and expensive. Featuretools is an open source python framework for automated feature engineering. For a comprehensive first step check out their [webpage](https://docs.featuretools.com/), for understand the math behind the magic check out this [paper!](http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf).

<center>
<img src = "https://camo.githubusercontent.com/cfcfc32dae79f7857d760a358227665a054b5583/68747470733a2f2f7777772e66656174757265746f6f6c732e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031372f31322f466561747572654c6162732d4c6f676f2d54616e676572696e652d3830302e706e67" height = 100>
</center>

* **[SHAP (SHapley Additive exPlanations) ](https://github.com/slundberg/shap)** - Model explainability is a priority in today’s data science community. **Shap** is a Python packages that connects game theory with local explanations to elucidate the output of any machine learning model.
 
 <center>
<img src = "https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/shap_diagram.png" height = 150>
</center>

* **[LightGBM](https://github.com/microsoft/LightGBM)**  - A fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. It is under the umbrella of the [DMTK](http://github.com/microsoft/dmtk) project of Microsoft.

<center>
<img src = "https://cdn-images-1.medium.com/max/800/1*mKkwlQF25Rq1ilne5UiEXA.png" height = 180>
</center>

 
* **[Hyperopt](https://github.com/hyperopt/hyperopt)** -  Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions. In this post, we will use **Hyperopt** to search the hyperparameters of **LightGBM** that better fit to the feature "Survived" (**Target**).

<center>
<img src="https://cdn-images-1.medium.com/max/800/1*1HhgVrhk7ABeEaLsTLbWHA.gif" height = 400>
</center>

* **[preml](https://github.com/csaybar/preml)** --  This is my Machine Learning toolkit. Here I'm putting my functions and the most useful auxiliary functions that I find when taking a walk for Kaggle. Clean documentation and reproducible examples are the most important in the construction of preml.

## 6. Workflow Stages
This script follows eight main parts:

- 0. Install, Load and check data
- 1. Data preparation
- 2. Model Implementation
- 3. Submission

For a reproducible example clic [**here**](https://colab.research.google.com/drive/1xiogjj0ciL2rsBe1bvmyub6EZs6r2y7c). 

### 6.0  Install, Load and Read data

#### 6.0.1 Install
```{python, eval = FALSE}
# Install packages 
# scikit-learn > 0.21 is necessary because this post use sklearn.impute.IterativeImputer
!pip install scikit-learn==0.21rc2
!pip3 install missingno
!pip install git+https://github.com/csaybar/preml.git --upgrade
!pip install category_encoders --upgrade
!pip install featuretools --upgrade
!pip install shap --upgrade
```

#### 6.0.2 Load

```{python, eval = FALSE}
# Future!
from __future__ import division, absolute_import, print_function

## Data science libraries
import pandas as pd # data structures and data analysis tools
import numpy as np # scientific computing
from matplotlib import pyplot as plt # fast-viz in python
import featuretools as ft

## csaybar machine learning toolkit!
import preml as pml 
from preml.utils import fast_view
from preml.utils import reduce_mem_usage
from preml.models import BayesianOptimization
from preml.plots import DUplots
from preml.preprocess import re_transform
from sklearn.pipeline import Pipeline

## Sklearn ecosystem <3!
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.impute import IterativeImputer
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
import category_encoders as ce # encode categorical variables as numeric.

## Hyperparameter Optimization
from hyperopt import hp, tpe
from hyperopt.fmin import fmin

# Basic python!
import os # Operating system interphases.
import gc #Garbage Collector Interface.
import time # handle time-related tasks.
from contextlib import contextmanager # utilities for with-statement contexts.
from collections import Counter # dict subclass for counting hashable objects.
import urllib

# GBDT frameworks! 
import lightgbm as lgbm  # gradient boosting framework-1.

# Model interpretation
import shap

pd.set_option('display.max_columns', 15) # Setting the max number of columns to display.
pd.set_option('display.max_rows',18) # Setting the max number of rows to display.
```

#### 6.0.3 Read

```{python, eval = FALSE}
# Download the dataset
url = 'https://raw.githubusercontent.com/csaybar/Titanic/master/'
urllib.request.urlretrieve(url+'train.csv', 'train.csv')
urllib.request.urlretrieve(url+'test.csv', 'test.csv')


#Let’s read in and take a peek at the data.
filenames = ('train.csv','test.csv')
dataset = pd.DataFrame()

for x in filenames:
  files = pd.read_csv(x)
  dataset = pd.concat([dataset,files],sort=False).reset_index(drop = True)  

basedataset = dataset.copy() # it will help us to compare after feature engineering
```

### 6.1Data Preparation

#### 6.1.1 Data understanding (DU)
Extracted from [Data mining for dummies](https://www.dummies.com/programming/big-data/phase-2-of-the-crisp-dm-process-model-data-understanding/)

DU is the **second phase** of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model, **you obtain data and verify that it is appropriate for your needs**. You might identify issues that cause you to return to business understanding and revise your plan. You may even discover flaws in your business understanding, another reason to rethink goals and plans.

**`pml.utils.fast_view`** is a function that automatically detects the dtype (numeric and object ) of the pd.DataFrame columns and  generate descriptive statistics that summarize the central tendency, dispersion, etc.

```{python, eval = FALSE}
object_table, numeric_table = fast_view(dataset)

# Changing data type
f_float = ['Fare', 'Age']
f_int = ['SibSp', 'Parch','Survived','Pclass','Parch']
f_category = ['Embarked', 'Ticket', 'Sex','Cabin']

dataset[f_float] = dataset[f_float].astype('float', errors = 'ignore')
dataset[f_int] = dataset[f_int].astype('int', errors = 'ignore')
dataset[f_category] = dataset[f_category].astype('category', errors = 'ignore')  


train = dataset.dropna(subset = ['Survived']) # train dataset
test = dataset[~ dataset.index.isin(train.index)] # test dataset
```

```{python, eval = FALSE}
object_table # Object table
```

<table class="table table-bordered table-hover table-condensed">
<thead><tr><th title="Field #1"> </th>
<th title="Field #2">Cabin</th>
<th title="Field #3">Embarked</th>
<th title="Field #4">Name</th>
<th title="Field #5">Sex</th>
<th title="Field #6">Ticket</th>
</tr></thead>
<tbody><tr>
<td>count</td>
<td>295</td>
<td>1307</td>
<td>1309</td>
<td>1309</td>
<td>1309</td>
</tr>
<tr>
<td>unique</td>
<td>186</td>
<td>3</td>
<td>1307</td>
<td>2</td>
<td>929</td>
</tr>
<tr>
<td>top</td>
<td>C23 C25 C27</td>
<td>S</td>
<td>Kelly, Mr. James</td>
<td>male</td>
<td>CA. 2343</td>
</tr>
<tr>
<td>freq</td>
<td>6</td>
<td>914</td>
<td>2</td>
<td>843</td>
<td>11</td>
</tr>
<tr>
<td>NaN_exist?</td>
<td>True</td>
<td>True</td>
<td>False</td>
<td>False</td>
<td>False</td>
</tr>
<tr>
<td>%perc_NA</td>
<td>0.775</td>
<td>0.002</td>
<td>0.0</td>
<td>0.0</td>
<td>0.0</td>
</tr>
</tbody></table>

```{python, eval = FALSE}
numeric_table # Numeric table
```

<table class="table table-bordered table-hover table-condensed">
<thead><tr><th title="Field #1"> </th>
<th title="Field #2">Age</th>
<th title="Field #3">Fare</th>
<th title="Field #4">Parch</th>
<th title="Field #5">PassengerId</th>
<th title="Field #6">Pclass</th>
<th title="Field #7">SibSp</th>
<th title="Field #8">Survived</th>
</tr></thead>
<tbody><tr>
<td>count</td>
<td align="right">1046.0</td>
<td align="right">1308.0</td>
<td align="right">1309.0</td>
<td align="right">1309.0</td>
<td align="right">1309.0</td>
<td align="right">1309.0</td>
<td align="right">891.0</td>
</tr>
<tr>
<td>mean</td>
<td align="right">29.9</td>
<td align="right">33.3</td>
<td align="right">0.4</td>
<td align="right">655.0</td>
<td align="right">2.3</td>
<td align="right">0.5</td>
<td align="right">0.4</td>
</tr>
<tr>
<td>std</td>
<td align="right">14.4</td>
<td align="right">51.8</td>
<td align="right">0.9</td>
<td align="right">378.0</td>
<td align="right">0.8</td>
<td align="right">1.0</td>
<td align="right">0.5</td>
</tr>
<tr>
<td>min</td>
<td align="right">0.2</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
</tr>
<tr>
<td>25%</td>
<td align="right">21.0</td>
<td align="right">7.9</td>
<td align="right">0.0</td>
<td align="right">328.0</td>
<td align="right">2.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
</tr>
<tr>
<td>50%</td>
<td align="right">28.0</td>
<td align="right">14.5</td>
<td align="right">0.0</td>
<td align="right">655.0</td>
<td align="right">3.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
</tr>
<tr>
<td>75%</td>
<td align="right">39.0</td>
<td align="right">31.3</td>
<td align="right">0.0</td>
<td align="right">982.0</td>
<td align="right">3.0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
</tr>
<tr>
<td>max</td>
<td align="right">80.0</td>
<td align="right">512.3</td>
<td align="right">9.0</td>
<td align="right">1309.0</td>
<td align="right">3.0</td>
<td align="right">8.0</td>
<td align="right">1.0</td>
</tr>
<tr>
<td>NaN_exist?</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">1.0</td>
</tr>
<tr>
<td>%perc_NA</td>
<td align="right">0.2</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.3</td>
</tr>
<tr>
<td>tukey_outlier_1.5</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">4.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">9.0</td>
<td align="right">0.0</td>
</tr>
<tr>
<td>zscore_outlier_7.5</td>
<td align="right">0.0</td>
<td align="right">4.0</td>
<td align="right">2.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
</tr>
<tr>
<td>shapiro_pvalue</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
</tr>
<tr>
<td>DAgostino_pvalue</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
</tr>
<tr>
<td>kstest_pvalue</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
<td align="right">0.0</td>
</tr>
<tr>
<td>Skew</td>
<td align="right">0.4</td>
<td align="right">4.4</td>
<td align="right">3.7</td>
<td align="right">0.0</td>
<td align="right">-0.6</td>
<td align="right">3.8</td>
<td align="right">0.5</td>
</tr>
<tr>
<td>Kurtosis</td>
<td align="right">0.1</td>
<td align="right">26.9</td>
<td align="right">21.5</td>
<td align="right">-1.2</td>
<td align="right">-1.3</td>
<td align="right">20.0</td>
<td align="right">-1.8</td>
</tr>
</tbody></table>


```{python, eval = FALSE}
# We will eliminate the variable Cabin because have a lot of np.NaN values.
dataset = dataset.drop(['Cabin'],axis=1)
```

NaN values existing in 5 of 12 base features, the more sophisticated imputation algorithms use [round-robin regression approaches](https://scikit-learn.org/stable/modules/impute.html).So, our special corplots (See **preml.plots.DUplots.corplot**) can we give us an initial idea of the imputation efficiency.


```{python, eval = FALSE}
from preml.plots import DUplots
du = DUplots(db_train = train,db_test = test)
du.corplot(remove = ['Survived','PassengerId'],fig_size=(15,5))
```

![](https://raw.githubusercontent.com/csaybar/Titanic/master/image1.png)


```{python, eval = FALSE}
du.missingplot(remove=['Survived'],figsize = (8,4))
```

![](https://raw.githubusercontent.com/csaybar/Titanic/master/image2.png)
![](https://raw.githubusercontent.com/csaybar/Titanic/master/image3.png)


Additionally, **`preml.plots.DUplots.missingplot()`** can account for the amount of missing data too.

```{python, eval = FALSE}
du.missingbar(remove=['Survived'],figsize = (8,4))
```

![](https://raw.githubusercontent.com/csaybar/Titanic/master/image4.png)
![](https://raw.githubusercontent.com/csaybar/Titanic/master/image5.png)

Now, we’ve got a better sense of our variables, their class type, NaN distribution, balances, etc. Here, a summary after see all features and plots:

Discoveries:

  - float (numeric) -> [Age,  Fare]
  - ordinal -> [Pclass, SibSp, Parch]
  - nominal -> [Embarked, Sex, Parch, Ticket, **Survived(TARGET)**]
  - The entire dataset have 1309 observations and 12 variables. 
  - The test dataset corresponds the 0.319 % of the entire dataset.  
  - The correlation between features is poor.
  - Fare have missing data in test but not in the training dataset.
  - More than the **20%** of the passengers do not have a age record.
  - More than the **75%** of the passengers do not have a Cabin record.
  - The missing data distribution is equal in the train as a test dataset.
  - **0.3838 %** of the passengers survived.
  - None of the features have a normal distribution.

#### 6.1.2 Splitting the name column in the Surnames and Title of Passengers
In my opinion (with a naked eye) after DU, the  more promising feature is the "**Name**"  column because it contains two important characteristics:

- The **surname** that can help us to represent families.
- The **passenger title**  a characteristic that was taken to prioritize survivors (Women and children first!). 

It's time fot a bit of **[regex](https://regexr.com/)**!

```{python, eval = FALSE}
print("Name: | " + " | ".join(dataset['Name'].tolist()))
title = dataset['Name'].str.extract('([A-Za-z]+)\.',expand=True)[0]
print("Title: " + " | ".join(title))
surname = dataset['Name'].str.extract('([A-Za-z]+)\,',expand=True)[0]
print("Surname: " + " | ".join(surname))

dataset['Title'] = title; dataset['Surname'] = surname
del dataset['Name']
```

In anyone ML pipeline is essential getting an excellent visual representation to find out hidden patterns in the data. To make it easily accessible, the Python package **preml** can also draws plots similar to [partial dependence plots](https://christophm.github.io/interpretable-ml-book/pdp.html),  but directly from data instead of using a trained model. This technique was inspired in the python package [featexp](https://github.com/abhayspawar/featexp), I have rewritten all the code and create a class (DUplots) for a more easily use.

 For example, **`preml.DUplots`** can help you to understand if the features `Title` vs. [`Age`, `Fare`] have some kind of relationship.
 
```{python, eval = FALSE}
from preml.plots import DUplots

#eliminate NA of the "Age" column.
dataset_age = dataset.dropna(subset=['Age'])


du = DUplots(db_train = dataset_age,             
             target='Age',
             features = ['Fare','Title'])
du.plot()
```

![](https://raw.githubusercontent.com/csaybar/Titanic/master/image6.png)
![](https://raw.githubusercontent.com/csaybar/Titanic/master/image7.png)
If it is a numeric features **`preml.DUplots`**   will create equal population bins (X-axis). Otherwise, if it is a categoric feature **`preml.DUplots`** will use the respective encoding. Always **`preml.DUplots`** will generate two plots. The first one,  calculates target mean ('Age') in each bin,  and the other,  is just a histogram. 

Inspecting our plots (see above),  the first row of figures (Age vs. Fare) say us nothing relevant. However, the second  saying us some important: **Passenger with the title of Master are CHILD!**, I am in doubt if all Master Passengers are male, to be sure we can run **`preml.DUplots`** one more time (like a little ingenuity).

**Psss: ** **`preml.DUplots`** can also return information as a table.

```{python, eval = FALSE}
du = DUplots(db_train = dataset_age[dataset_age.Title == 'Master'],
             target='Age',
             features = ['Sex'])
du.plot()

du.tables()['Sex']
```

![](https://raw.githubusercontent.com/csaybar/Titanic/master/image8.png)


<table class="table table-bordered table-hover table-condensed">
<thead><tr><th title="Field #1">Sex</th>
<th title="Field #2">Samples_in_bin</th>
<th title="Field #3">Age_mean</th>
</tr></thead>
<tbody><tr>
<td>female</td>
<td align="right">0</td>
<td>NaN</td>
</tr>
<tr>
<td>male</td>
<td align="right">53</td>
<td>5.482642</td>
</tr>
</tbody></table>

We got it!, Now you know that "Master" Title refers to boys and knows how **`preml.DUplots`** works! We will create more feature in the next section.

#### 6.1.3 Generalization of the "Title" feature

```{python, eval = FALSE}
def newTitle(dataset):
  order_name = {'Mlle': 'Mrs', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr',
                'Don': 'Mr', 'Mme': 'Mrs','Jonkheer': 'Mr', 'Lady': 'Mrs',
                'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Mrs', 'Dona': 'Mrs',
                'Miss': 'Mrs', 'Mr':'Mr', 'Mrs':'Mrs', 'Master':'boy', 'Dr':'Mr',
                'Rev': 'Mr'              
                }
  #Drs females
  Title = dataset.Title.map(order_name)
  dr_females = dataset[(dataset['Sex']=='female') & (dataset['Title']=='Dr')]
  Title[dr_females.index[0]] = 'Mrs'
  return Title
```

```{python, eval = FALSE}
dataset['Title'] = newTitle(dataset)
```

#### 6.1.4 Missing data
To handle missing data  existing two groups of imputation algorithm:
- **univariate**, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension.
- **multivariate** , which imputes values  considering the entire set of available feature dimensions to estimate the missing values.

**Sklearn**  has recently implemented the multivariate approach in the class [**IterativeImputer**](https://scikit-learn.org/dev/modules/generated/sklearn.impute.IterativeImputer.html), which models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.

You can learn how implement it checking the **ImputeTitanic_db** function.


```{python, eval = FALSE}
def ImputeTitanic_db(dataset):
  db = dataset.drop(['Surname','PassengerId','Survived','Ticket'],axis=1)
  y = dataset['Survived']
  # 2.4.1. Bayesian Target_Encoding 
  te = ('te', ce.TargetEncoder(handle_missing= 'return_nan',drop_invariant = False))
  # 2.4.2. Iterative Imputer (using ExtraTreesRegressor)
  RANDOM_STATE = 100
  imputer_estimator = ExtraTreesRegressor(random_state = RANDOM_STATE,n_estimators=100)
  imputation = ('ii',IterativeImputer(random_state = RANDOM_STATE, estimator = imputer_estimator))
  preprocess = Pipeline([te,imputation])
  impute_db = pd.DataFrame(preprocess.fit_transform(db,y))
  impute_db.columns = db.columns
  
  col_obj = dataset[['Surname','PassengerId','Survived','Ticket']]
  col_num = re_transform(old_db = db,new_db = impute_db,retransform = ['Fare','Embarked','Title','Sex'])
  all_db = pd.concat([col_obj,col_num],axis=1)
  
  return all_db
```

```{python, eval = FALSE}
dataset = ImputeTitanic_db(dataset)
```

#### 6.1.5 Adding Nannies to the family!

Idea extracted from [Jack Roberts ](https://www.kaggle.com/jack89roberts/titanic-using-ticket-groupings) and [Erik Bruin's](https://www.kaggle.com/erikbruin/titanic-2nd-degree-families-and-majority-voting/report) .
<center>
  <img src='https://drive.google.com/uc?export=view&id=1bXJZSwODFoK6TGGR3yMzcAra30QgAN2M' >
</center>

You can think that all families (Surname) travel together, but this is not 100% true. Surname doesn't always imply that passengers are in the same family and traveling together. It is important because their probability of **Survived** will not be the same!. There are many kernels than explains how to create robust  **Passenger groups**  ([here](https://www.kaggle.com/francksylla/titanic-machine-learning-from-disaster) or [here](https://www.kaggle.com/erikbruin/titanic-2nd-degree-families-and-majority-voting/report)). While the creation of a "pure" group can improve more your score in LB, I limit to regroup families considering nannies (if she will exist!).

```{python, eval = FALSE}
# 6.1.5.1 Select the possible nannies
possible_nannies = (dataset[(dataset.groupby('Surname')['Surname'].transform('count') == 1)]
                    .query("Sex == 'female'")
                    .index)

# 6.1.5.1 Families with childs (< 15 years)
bigFamilies = (dataset[(dataset.groupby('Surname')['Surname'].transform('count') > 1)]) 
bigFamilies_le15 = bigFamilies[bigFamilies.groupby('Surname')['Age'].transform('min') < 15]
bigFamilies_le15_m = bigFamilies_le15.groupby('Surname')[['Surname','Ticket']].agg(lambda x: pd.Series.mode(x)[0])
bigFamilies_le15_m = bigFamilies_le15_m.reset_index(drop=True)


for x in possible_nannies:
  # Comparate the ticket of each possible nanny vs. all tickets families
  pot_position = np.where(bigFamilies_le15_m['Ticket'] == dataset.loc[x,'Ticket'])[0]
  if sum(pot_position) :    
    #Changing the nanny Surname
    dataset.loc[x,'Surname'] = bigFamilies_le15_m.loc[pot_position[0],'Surname']    
```

#### 6.1.6 Family Size

```{python, eval = FALSE}
dataset['Family_Size'] = dataset.groupby('Surname')['Surname'].transform('count')
```

#### 6.1.7 Fare by Family

```{python, eval = FALSE}
dataset['FbFare'] = dataset['Fare']*dataset['Family_Size']
```

#### 6.1.8 Woman-child-group (WCG) Feature 

WCG Is a feature firstly proposed by [Chris Deotte](https://www.kaggle.com/cdeotte) [here](https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818). The idea behind this is to use an empirical tree that takes into account the "**Surname**", "**Title**" and "**Survived**" features. The dataset we will split in three as follows:

- **The WCG dataset**: Estimate using just the WCG (without model).
- **Female Dataset**: A dataset of women.
- **Male Dataset**: A dataset of men.
  
<center>
  <img src='https://drive.google.com/uc?export=view&id=17sqMRdVzdDwfg3XAQcTWoegDcvgnoPmo' >
</center>

```{python, eval = FALSE}
def create_WCG(dataset):
  # WCG feature
  surname = pd.DataFrame({'Surname':dataset['Surname']})
  family_size = surname.groupby('Surname')['Surname'].transform('count')
  surname.loc[(family_size<=1) | (dataset['Title'] == 'Mr'),'Surname'] = 'nogroup'
  surname['Survived'] = dataset['Survived']
  surname['Title'] = dataset['Title']
  surname['WCG'] =  surname.groupby('Surname')['Survived'].transform('mean')
  
  surname['WCG_p'] = 0
  
  surname.loc[surname['Title']=='Mrs','WCG_p'] = 1
  surname.loc[(surname['Title']=='boy') & (surname['WCG']==1),'WCG_p'] = 1
  surname.loc[(surname['Title']=='Mrs') & (surname['WCG']==0),'WCG_p'] = 0
  
  
  surname['split'] = 'model'
  surname.loc[(surname['Title']=='boy') & (surname['WCG']==1),'split'] = 'WCG'
  surname.loc[(surname['Title']=='Mrs') & (surname['WCG']==0),'split'] = 'WCG'

  return surname['WCG_p'], surname['split']
```


```{python, eval = FALSE}
dataset['WCG'], dataset['split']= create_WCG(dataset)
```

The efficiency of WCG can be measure by **`du.plot()`** and **`du.table()`**

```{python, eval = FALSE}
# @title Duplot
dataset['WCG'] =  dataset['WCG'].astype('category')
du = DUplots(db_train = dataset[dataset['Survived'].notnull()],
             target='Survived',
             features = ['WCG'])
du.plot()
du.tables()
```

![](https://raw.githubusercontent.com/csaybar/Titanic/master/image9.png)

This feature itself can reach an impressive 89% (85% ) of accuracy for predict survived (die) in training dataset. **Unbelievable!**

### 6.2 Model Implementation

WCG is a great feature, but there is a big group that only predicts died (0) if the Passenger is a Male or lived (1) if the Passenger is a Female (See diagram above). In this section, you will replace this part by a ML model (Lightgbm).

#### 6.2.1 Splitting the dataset

```{python, eval = FALSE}
features = ['PassengerId','Pclass','Age','Family_Size','FbFare'] # choose the features to do the prediction

# Dataset WCG
dataset_wcg = dataset[(dataset['split'] == 'WCG') & dataset['Survived'].isnull()]
dataset_wcg = dataset_wcg[['PassengerId','WCG']]
dataset_wcg.columns = ['PassengerId','Survived']
```


```{python, eval = FALSE}
# Dataset male 
dataset_male = dataset[(dataset['split'] == 'model') & (dataset['Sex'] == 'male')]

# Male train
dataset_male_train = dataset_male[dataset_male['Survived'].notnull()]
X_train_male = dataset_male_train[features]
y_train_male = dataset_male_train['Survived']

# Male test
dataset_male_test = dataset_male[dataset_male['Survived'].isnull()]
X_test_male = dataset_male_test[features]

PassengerId_male = dataset_male_test['PassengerId'].reset_index(drop=True)
```

```{python, eval = FALSE}
# Dataset female
dataset_female = dataset[(dataset['split'] == 'model') & (dataset['Sex'] == 'female')]

# Female train
dataset_female_train = dataset_female[dataset_female['Survived'].notnull()]
X_train_female = dataset_female_train[features]
y_train_female = dataset_female_train['Survived']

# Female test
dataset_female_test = dataset_female[dataset_female['Survived'].isnull()]
X_test_female = dataset_female_test[features]

PassengerId_female = dataset_female_test['PassengerId'].reset_index(drop=True)
```

```{python, eval = FALSE}
#### 6.2.2 Feature Engineering with featuretools

In this post I do not cover all the details related to featuretools but is you are interesting, I highly recommend looking at all the  [official demos](https://www.featuretools.com/demos/) and this [fantastic kernel](https://www.kaggle.com/liananapalkova/automated-feature-engineering-for-titanic-dataset).
```


```{python, eval = FALSE}
def create_titanic_entity(df,ID_index, entity_name = 'EntitySet', bei = 'train'):
  es = ft.EntitySet(id = entity_name)
  es = es.entity_from_dataframe(entity_id = 'train', dataframe = df, 
                              variable_types = {
                                  'PassengerId': ft.variable_types.Index
                              },
                              index = ID_index)    
  es = es.normalize_entity(base_entity_id=bei, new_entity_id='Pclass', index='Pclass')
  es = es.normalize_entity(base_entity_id=bei, new_entity_id='FbFare', index='FbFare')
  es = es.normalize_entity(base_entity_id=bei, new_entity_id='Family_Size', index='Family_Size')  
  return es

def fe_titanic(df,ID_index='PassengerId',entity_name = 'Titanic',bei='train', max_depth = 2):
  es = create_titanic_entity(df, ID_index = ID_index, entity_name = entity_name, bei=bei)
  
  features, feature_names = ft.dfs(entityset = es,
                                   target_entity = bei,
                                   max_depth = max_depth)
  return (es,features, feature_names)
```


```{python, eval = FALSE}
es, X_train_male_ft, _ = fe_titanic(X_train_male)
es, X_test_male_ft, _ = fe_titanic(X_test_male)

es, X_train_female_ft, _ = fe_titanic(X_train_female)
es, X_test_female_ft, _ = fe_titanic(X_test_female)
```


```{python, eval = FALSE}
es.plot() #relationships between tables
```

![](https://raw.githubusercontent.com/csaybar/Titanic/master/images10.png)

#### 6.2.3 Feature Selection considering correlation and  shapley values

In the next cell, you use the function **select_ft** to get the most critical features considering correlations each other and **SHAP**.

In a nutshell, **[SHAP](https://github.com/slundberg/shap)** use game theory to interpret the target model. All features are “contributor” and trying to predict the task which is “game” and the “reward” is actual prediction minus the result from explanation model. **SHAP** unlike standard global feature importance measures (weight, cover, and gain) provide **consistent** (whenever we change a model the attributed importance for that feature should not decrease) and **accuracy** (the sum of all the feature importances should sum up to the total importance of the model).

```{python, eval = FALSE}
def VI_shapley(shap_values,X_train):
  shap_sum = np.abs(shap_values).mean(axis=0)
  importance_df = pd.DataFrame([X_train.columns.tolist(), shap_sum.tolist()]).T
  importance_df.columns = ['column_name', 'shap_importance']
  importance_df = importance_df.sort_values('shap_importance', ascending=False)
  importance_df['shap_importance'] = importance_df['shap_importance'].astype('float')
  return importance_df

def select_ft(model, X, y,n_ft = 5 ,cor_thr = 0.9):
  model.fit(X,y)
  #SHAP
  explainer = shap.TreeExplainer(model)
  shap_values = explainer.shap_values(X)
  vi_shp = VI_shapley(shap_values,X)
  best_features = vi_shp.sort_values('shap_importance',ascending=False).reset_index(drop=True)
  
  #CORR
  initial_ft = best_features.iloc[0:n_ft,0].tolist() #Getting the most Importance (according to shap)
  
  #Create a lower triangle from a correlation matrix
  cormtx = X[initial_ft].corr()
  mask = np.zeros_like(cormtx, dtype=np.bool)
  mask[np.triu_indices_from(mask)] = True
  cormtx[mask]=np.NaN
  
  
  if sum((cormtx>cor_thr).values.flatten()):
    cor_info = cormtx.reset_index().melt('index')  
    print('A high correlation between features was found!')
    print(cor_info[cor_info.value>cor_thr])
  return initial_ft
```


```{python, eval = FALSE}
model =lgbm.LGBMClassifier(boosting_type='dart', random_state=10)
fi_male = select_ft(model,X_train_male_ft,y_train_male)
#fi_male.remove('FbFare.MODE(train.Pclass)')
fi_female = select_ft(model,X_train_female_ft,y_train_female)
```

#### 6.2.4 Bayesian Optimization for getting Hyperparameters!

Optimization tries to find the set the parameters that minimize a specific function. The mentioned situation would appear easy to solve (Grid Search and Random Search), but the question makes complicated when we work with tens or hundreds of parameters and the model to evaluate is computationally expensive.

Bayesian optimization (implemented in the **[Hyperopt](https://github.com/hyperopt/hyperopt)** Python packages) is a sequential algorithm that takes advantage of the prior stage to make future predictions.  Understand the [mathematical behind this algorithm](http://proceedings.mlr.press/v28/bergstra13.pdf) can be a little intimidating. However, the **Hyperopt API** is really easy to use you just need the follows:

- **Objective Function**: takes in an input and returns a loss to minimize
- **Domain space**: the range of input values to evaluate
- **Optimization Algorithm**: the method used to construct the surrogate function and choose the next values to evaluate
- **Results**: score, value pairs that the algorithm uses to build the model

The **`preml.models.BayesianOptimization`** joins these four parts in just one class!. See the documentation.
 
```{python, eval = FALSE}
def bayesian_optim(model,X,y):
  
  model_params = [('hp.quniform',{'label':'model__num_leaves', 'low':3,'high':100,'q':1},'int'),
                  ('hp.quniform',{'label':'model__max_depth', 'low':3,'high':50,'q':1},'int'),
                  ('hp.uniform',{'label':'model__colsample_bytree', 'low':0.01,'high':1.0},'float'),
                  ('hp.uniform',{'label':'model__min_child_weight', 'low':1,'high':10},'float'),
                  ('hp.quniform',{'label':'model__max_bin', 'low':10,'high':1000,'q':10},'int')                                   
                 ]

  crossval_params = {'n_splits':5,'n_repeats':5,'random_state':100}
  cv = RepeatedStratifiedKFold(**crossval_params)
  
  cv_params = {'estimator':model,
               'X':X,
               'y':y,
               'scoring':'roc_auc',
               'cv':cv}

  byopt_model = BayesianOptimization(params=model_params,
                                     max_evals=50,
                                     cv_params=cv_params,
                                     cv_stat=np.median)

  byopt_model.search()
  return cv_params['estimator']
```


```{python, eval=FALSE}
def ROC_AUCplot(probs,y):
  'Plotting the ROC curve'
  fpr, tpr, thresholds = roc_curve(y,probs[:, 1])
  
  plt.plot([0, 1], [0, 1], linestyle='--')
  # plot the roc curve for the model
  plt.plot(fpr, tpr, marker='.')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver operating characteristic example')
  plt.legend(loc="lower right")
  # show the plot
  pyplot.show()
```

```{python, eval=FALSE}
def optim_acc_score(model,data,target,iterations = 100, rgn = [0,1]):
  """
  optim acc-score
  arguments:
    - model      : Put your model.
    - data       : X_values
    - target     : y_values
    - iterations : Number of iteration (Force brute)
    - rgn        : Range searching for f1-threshold.
  return:
    Best f1-score found
  """
  def optim_model(x):
    pred_df = pd.DataFrame(model.predict_proba(data))
    y_pred = (pred_df.iloc[:,1].values > x)*1
    return accuracy_score(target, y_pred)
  rango = np.linspace(rgn[0], rgn[1], iterations)
  accuracy_scr = pd.Series([optim_model(z) for z in rango])
  return rango[accuracy_scr.idxmax()]
```

- **Female Dataset Prediction**

```{python, eval=FALSE}
# 6.2.1.1 Select variables considering the section 6.2.3
X_train_female_ft_fs = X_train_female_ft[fi_female]
female_model = Pipeline([('scale',StandardScaler()),
                    ('model',lgbm.LGBMClassifier(boosting_type='dart'))])

# 6.2.1.2 Bayesian optimization
female_model_optim = bayesian_optim(female_model,X_train_female_ft[fi_male],y_train_female)
female_model_optim.fit(X_train_female_ft_fs,y_train_female)

ROC_AUCplot(probs = female_model_optim.predict_proba(X_train_female),
            y = y_train_female.values)

# 6.2.1.3 Optim accuracy score
op_thrs = optim_acc_score(female_model_optim,X_train_female,y_train_female)
female_predictions = (female_model_optim.predict_proba(X_test_female)[:,1] > op_thrs)*1

# 6.2.1.4 Make a pd.DataFrame for submission
female_submission = pd.concat([PassengerId_female,pd.Series(female_predictions)],axis=1)
female_submission.columns = ['PassengerId','Survived']
```

- **Male Dataset Prediction**

```{python, eval=FALSE}
# 6.2.1.5 Select variables considering the section 6.2.3
X_train_male_ft_fs = X_train_male_ft[fi_male]
male_model = Pipeline([('scale',StandardScaler()),
                    ('model',lgbm.LGBMClassifier(boosting_type='dart'))])

# 6.2.1.2 Bayesian optimization
male_model_optim = bayesian_optim(male_model,X_train_male_ft_fs,y_train_male)
male_model_optim.fit(X_train_male_ft_fs,y_train_male)

ROC_AUCplot(probs = male_model_optim.predict_proba(X_train_male),
            y = y_train_male.values)

# 6.2.1.3 Optim accuracy score
op_thrs = optim_acc_score(male_model_optim,X_train_male,y_train_male)
male_predictions = (male_model_optim.predict_proba(X_test_male)[:,1] > op_thrs)*1

# 6.2.1.4 Make a pd.DataFrame for submission
male_submission = pd.concat([PassengerId_male,pd.Series(male_predictions)],axis=1)
male_submission.columns = ['PassengerId','Survived']
```

### 6.3 Create Submission

```{python, eval=FALSE}
submission = pd.concat([female_submission,male_submission,dataset_wcg]).sort_index()
submission['Survived'] = submission['Survived'].astype(int)
submission.sort_values(['PassengerId'],inplace=True)
submission.to_csv('submission.csv',index=False)
```

### 6.4 Conclusion

This notebook demonstrated (see ROC curves) that **lightgbm** does not make a better job than WCG for predict women and adults male survivals. 

I think the reasons are:
  - Pclass, Age, Fare do not provide any explanatory power to the model.
  - Survive to the disaster is a question of luck in the majority of the time.
  - Ensemble methods such as **Lightgbm** works better under large datasets.

In other posts (like [this](https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688))  has been demonstrated that XGBoost jointly with a better separation of the dataset can achieve until 85% on the LB, so it is an excellent option to keep these posts in mind as well.

![dd](https://raw.githubusercontent.com/csaybar/Titanic/master/my_position.png)
