---
title: "Integrating Earth Engine with Tensorflow II - U-Net"
author: "Cesar Aybar" 
date: "2019-06-21"
banner: "img/banners/05_banner_ee.png"
tags: ["Python", "tensorflow", "keras", "GEE"]
---



<center>
<a href="https://colab.research.google.com/github/csaybar/EEwPython/blob/master/cnn_demo.ipynb"><img align="left" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab" title="Open and Execute in Google Colaboratory"></a>
</center>
<p><br></p>
<p>This notebook has been inspired by the <a href="https://www.youtube.com/watch?v=w-1xfF0IaeU">Chris Brown &amp; Nick Clinton EarthEngine + Tensorflow presentation</a>. It shows the step by step how to integrate Google Earth Engine and TensorFlow 2.0 in the same pipeline (EE-&gt;Tensorflow-&gt;EE).</p>
<center>
<img src="https://raw.githubusercontent.com/csaybar/EEwPython/master/images/colab_ee_integration.png">
</center>
<p>OBS: I will assume reader are already familiar with the basic concepts of Machine Learning and Convolutional Networks. If it is doesn’t, I firstly highly recommend taking the deep learning coursera specialization available <a href="https://www.coursera.org/specializations/deep-learning">here.</a></p>
<div id="topics" class="section level2">
<h2>Topics</h2>
<ol style="list-style-type: decimal">
<li>Create a training/testing dataset (in a TFRecord format) using Earth Engine.</li>
<li>Create functions for parse data (TFRecord -&gt; tf.data.Dataset; Decode the bytes into an image format).</li>
<li>Shuffle, repeat and batch the data.</li>
<li>Training and Test a Convolutional Neuronal Network using tensorflow 2.0.</li>
<li>Making predictions on image data exported from Earth Engine in TFRecord format.</li>
<li>Upload your results to Earth Engine (asset).</li>
</ol>
</div>
<div id="introduction" class="section level2">
<h2>1. Introduction</h2>
<p>Deep learning has dramatically improved the state-of-the-art in various science domains. For remote sensing, its potential has not been thoroughly explored yet. This could be related to the problematic incorporation of spectral &amp; spatial features into a regular deep learning classification scheme or for the huge pre-processing that satellite images could need it.</p>
<p>Hence, this post aims to teach you how to create a painless deep learning workflow integrating <a href="https://earthengine.google.com/">Google Earth engine</a> for acquiring spectral &amp; spatial data and <a href="https://www.tensorflow.org/?hl=en">tensorflow</a> for train and test the model and make predictions.</p>
</div>
<div id="what-is-google-earth-engine-gee" class="section level2">
<h2>2. What is Google Earth Engine (GEE)?</h2>
<p>In a nutshell, is a platform that combines a multi-petabyte <a href="https://developers.google.com/earth-engine/datasets/">catalog of satellite imagery</a> with planetary-scale analysis capabilities. There are several ways to interact with GEE:</p>
<ul>
<li><a href="https://explorer.earthengine.google.com/">Explorer</a></li>
<li><a href="https://code.earthengine.google.com/">Code Editor</a><br />
</li>
<li><a href="https://github.com/google/earthengine-api/tree/master/javascript">Javascript wrapper library</a></li>
<li><a href="https://github.com/google/earthengine-api/tree/master/python"><strong>Python wrapper library</strong></a></li>
</ul>
<p>In my opinion, the <strong>Python wrapper library</strong> (used in this post) is the best choice to interact with GEE for the following reasons:</p>
<ul>
<li>Easy to share code.</li>
<li>Easy transition to a web application.</li>
<li>Possibility to integrate with ML/DL frameworks.</li>
<li>Many plotting options (folium, plotly, matplotlib, seaborn ,etc.).</li>
</ul>
<p>One more thing!, it’s possible to run the Earth Engine Python API in a cloud environment for free. See the introduction of this <a href="https://colab.research.google.com/github/csaybar/EEwPython/blob/dev/1_Introduction.ipynb">course</a> for more details.</p>
</div>
<div id="u-net-convolutional-networks-for-biomedical-image-segmentation---adapted-from-the-harshall-lamba-post" class="section level2">
<h2>3. U-net: Convolutional Networks for Biomedical Image Segmentation - Adapted from the <a href="https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47">Harshall Lamba post</a></h2>
<p>There are various levels of granularity in which the computers can gain an understanding of images. For each of these levels there is a problem defined in the <a href="https://en.wikipedia.org/wiki/Computer_vision">Computer Vision</a> domain. Starting from a coarse grained down to a more fine grained understanding we can identify the following problems.</p>
<ul>
<li>Classification with Localization</li>
<li>Object Detection</li>
<li><strong>Semantic Segmentation</strong></li>
<li>Instance segmentation</li>
</ul>
<center>
<image src="https://cdn-images-1.medium.com/max/800/1*SNvD04dEFIDwNAqSXLQC_g.jpeg" width=50%>
</center>
As you can see, a typical remote sensing classification task search the same thing than
a semantic segmentation problem. One of the most popular state-of-art CNN used in semantic segmentation is the <strong>U-Net</strong> (it will be used in this post) initially developed for biomedical image interpretation. This network is a slight modification of the <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">fully convolutional network (FCN)</a>.
Inside the U-Net architecture exist two contrasting parts (see image below). The first one is the contraction path (also called as <strong>encoder</strong>) which is used to capture the context in the image. The encoder is just a traditional stack of convolutional and max-pooling layers. The second path is the symmetric expanding path (also called as <strong>decoder</strong>) which is used to enable precise localization using transposed convolutions. Unlike the original model, we will add <a href="https://en.wikipedia.org/wiki/Batch_normalization">batch normalization</a> to each of our blocks.<br />

<center>
<image src="https://cdn-images-1.medium.com/max/1600/1*OkUrpDD6I0FpugA_bbYBJQ.png" width=70%>
</center>
<p>The main steps for building a U-net are:</p>
<ol style="list-style-type: decimal">
<li>Create the dataset considering patch of 256x256.</li>
<li>Visualize data/perform some exploratory data analysis.</li>
<li>Set up data pipeline and preprocessing.</li>
<li>Initialize the model’s parameters.</li>
<li>Loop:
<ul>
<li>Calculate current loss (forward propagation) :<br />
</li>
<li>Calculate current gradient (backward propagation)</li>
<li>Update parameters (gradient descent)</li>
</ul></li>
</ol>
<p>The five step could be a little intimidating, but don’t worry about it!. <strong>tf.keras</strong>, the TensorFlow’s high-level API, only need that you define the forward propagation correctly and all the steps further down will make automatically. This post does not intend to introduce the algorithm, check out this <a href="https://github.com/jakeret/tf_unet">repo</a> for a from zero (tensorflow) implementation.</p>
</div>
<div id="crop-area-estimation-in-camana-valley-demo" class="section level2">
<h2>4. Crop Area estimation in Camana Valley (DEMO)</h2>
<p>Agriculture is part of the backbone Peruvian economy, contributing about 7.6% of the Gross Domestic Product (GDP), being more critical in rural areas where the contribution of GDP increase until 50%. In terms of people, this activity act for the primary source of income for 2.3 million families, representing 34% of Peruvian households. Despite agriculture importance in Peruvian family lives, today no exist a cropping system either at a national or regional scale that monitoring the extension, state, or crop type. Considering this problematic, in this section <strong>you</strong> will create a straightforward methodology to <strong>predict the crop area</strong> in Camana (Arequipa) Valley using the CNN above described.</p>
<center>
<img src='https://st.depositphotos.com/1171712/3974/i/950/depositphotos_39741899-stock-photo-camana-valley.jpg'>
</center>
<pre class="python"><code>#Mapdisplay: Display ee.Features and ee.Images using folium.

def Mapdisplay(center, dicc, Tiles=&quot;OpensTreetMap&quot;,zoom_start=10):
    &#39;&#39;&#39;
    :param center: Center of the map (Latitude and Longitude).
    :param dicc: Earth Engine Geometries or Tiles dictionary
    :param Tiles: Mapbox Bright,Mapbox Control Room,Stamen Terrain,Stamen Toner,stamenwatercolor,cartodbpositron.
    :zoom_start: Initial zoom level for the map.
    :return: A folium.Map object.
    &#39;&#39;&#39;
    mapViz = folium.Map(location=center,tiles=Tiles, zoom_start=zoom_start)
    for k,v in dicc.items():
      if ee.image.Image in [type(x) for x in v.values()]:
        folium.TileLayer(
            tiles = EE_TILES.format(**v),
            attr  = &#39;Google Earth Engine&#39;,
            overlay =True,
            name  = k
          ).add_to(mapViz)
      else:
        folium.GeoJson(
        data = v,
        name = k
          ).add_to(mapViz)
    mapViz.add_child(folium.LayerControl())
    return mapViz</code></pre>
<div id="installing" class="section level3">
<h3>4.1. Installing</h3>
<p>Before coding does not forget install and load the following packages and remember that you can communicate with the bash console prepending an ! to the code.</p>
<pre class="python"><code>!pip install tf-nightly-2.0-preview==2.0.0.dev20190606  #tensorflow 2.0
!pip install earthengine-api==0.1.175 #earthengine API
# Load the TensorBoard notebook extension
%load_ext tensorboard</code></pre>
</div>
<div id="authentification" class="section level3">
<h3>4.2. Authentification</h3>
<p>This tutorial needs interacting with some Google services. For accomplish this task, it’s necessary to authenticate (as yourself). The code below shows you how to do it.</p>
<div id="google-cloud" class="section level4">
<h4>Google Cloud</h4>
<p>Google Cloud Storage bucket will serve as a bridge between GEE and Colab.</p>
<pre class="python"><code>from google.colab import auth
auth.authenticate_user()</code></pre>
</div>
<div id="google-earth-engine" class="section level4">
<h4>Google Earth Engine</h4>
<pre class="python"><code>!earthengine authenticate</code></pre>
</div>
</div>
<div id="initialize-and-testing-the-software-setup" class="section level3">
<h3>4.3. Initialize and testing the software setup</h3>
<pre class="python"><code># Earth Engine Python API
import ee 
ee.Initialize()

import tensorflow as tf
print(&#39;Tensorflow version: &#39; + tf.__version__)

import folium
print(&#39;Folium version: &#39; + folium.__version__)

# Define the URL format used for Earth Engine generated map tiles.
EE_TILES = &#39;https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}&#39;</code></pre>
</div>
<div id="prepare-the-dataset" class="section level3">
<h3>4.4. Prepare the Dataset</h3>
<p>Firstly, we define our prediction area (Camana Valley) and passing to GEE. For moving a vector to GEE, you will use the <code>ee.Geometry.*</code> module. The <a href="https://geojson.org/">GeoJSON</a> spec describes in detail the type of geometries supported by GEE, including <code>Point</code> (a list of coordinates in some projection), <code>LineString</code> (a list of points), <code>LinearRing</code> (a closed LineString), and <code>Polygon</code> (a list of LinearRings where the first is a shell and subsequent rings are holes). GEE also supports <strong>MultiPoint</strong>, <strong>MultiLineString</strong>, and <strong>MultiPolygon</strong>. The <a href="https://geojson.org/">GeoJSON</a> GeometryCollection is also supported, although it has the name <strong>MultiGeometry</strong> within GEE.</p>
<pre class="python"><code># 4.4.1 Prediction Area
xmin,ymin,xmax,ymax = [-72.778645, -16.621663, -72.66865, -16.57553]


# Passing a rectangle (prediction area) to Earth Engine
Camana_valley = ee.Geometry.Rectangle([xmin,ymin,xmax,ymax])
center = Camana_valley.centroid().getInfo()[&#39;coordinates&#39;]
center.reverse()
Mapdisplay(center,{&#39;Camana Valley&#39;:Camana_valley.getInfo()},zoom_start=12)</code></pre>
<center>
<img src="https://raw.githubusercontent.com/csaybar/csaybar.github.io/master/img/unet_01.png">
</center>
<p>Next, you will read and create a visualization of the train/test dataset. I’ve already generated some points with the label agriculture/non-agriculture.</p>
<ul>
<li>Train dataset (550 points):
<ul>
<li>275 labeled as “agriculture”</li>
<li>275 labeled as “non agriculture”</li>
</ul></li>
<li>Test dataset (100 points):
<ul>
<li>50 labeled as “agriculture”</li>
<li>50 labeled as “non agriculture”</li>
</ul></li>
</ul>
<pre class="python"><code># 4.4.2 Importing the train/test dataset
train_agriculture = ee.FeatureCollection(&#39;users/csaybar/DLdemos/train_set&#39;) 
test_agriculture = ee.FeatureCollection(&#39;users/csaybar/DLdemos/test_set&#39;)

# Display the train/test dataset
db_crop = train_agriculture.merge(test_agriculture)
center = db_crop.geometry().centroid().getInfo()[&#39;coordinates&#39;]
center.reverse()

dicc = {&#39;train&#39;: train_agriculture.draw(**{&#39;color&#39;: &#39;FF0000&#39;, &#39;strokeWidth&#39;: 5}).getMapId(),
        &#39;test&#39; : test_agriculture.draw(**{&#39;color&#39;: &#39;0000FF&#39;, &#39;strokeWidth&#39;: 5}).getMapId(),
        &#39;CamanaValley&#39;:Camana_valley.getInfo()
       }

Mapdisplay(center,dicc,zoom_start=8)</code></pre>
<center>
<img src="https://raw.githubusercontent.com/csaybar/csaybar.github.io/master/img/unet_02.png">
</center>
<p>For train the model, unlike the <a href="https://csaybar.github.io/post/eetf/">first post</a>, you will use the <strong>Cultivated Lands class (raster)</strong> of the <a href="https://www.sciencedirect.com/science/article/pii/S0924271614002275">GLC30</a>, it is a relative new Global Land Cover product at 30-meter spatial resolution.</p>
<pre class="python"><code>from collections import OrderedDict

# Load the dataset
glc30 = ee.ImageCollection(&#39;users/csaybar/GLC30PERU&#39;).max().eq(10).rename(&#39;target&#39;)

# Vizualize the dataset

glc30id = glc30.getMapId()
dicc[&#39;glc30&#39;] = glc30id
# Changing the order of the dictionary
key_order = [&#39;glc30&#39;,&#39;CamanaValley&#39;,&#39;train&#39;,&#39;test&#39;]
dicc = OrderedDict((k, dicc[k]) for k in key_order)

Mapdisplay(center,dicc,zoom_start=8)</code></pre>
<center>
<img src="https://raw.githubusercontent.com/csaybar/csaybar.github.io/master/img/unet_03.png">
</center>
<p>At this part, you will obtain the input data for mapping the <strong>Camana crop area</strong> from <a href="https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C01_T1_SR">Landsat ETM sensor (L5)</a>. GEE provides L5 images with radiometric and geometry correction. Additionally, <strong>cloud mask information</strong> is supplied employing the bit image <code>pixel_qa</code>. The following function allows putting NA to the TOA reflectance values of clouds.</p>
<pre class="python"><code>def maskS2clouds(img):
  &#39;&#39;&#39;  
  Function to mask clouds based on the pixel_qa band of Landsat 5 data. See:
  https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LT05_C01_T1_SR
  
  Params:
  -------
  - img: image input Landsat 5 SR image
  
  Return:
  -------
  cloudmasked Landsat 5 image
  &#39;&#39;&#39;
  qa = img.select(&#39;pixel_qa&#39;)
  cloud = qa.bitwiseAnd(1 &lt;&lt; 5)\
            .And(qa.bitwiseAnd(1 &lt;&lt; 7))\
            .Or(qa.bitwiseAnd(1 &lt;&lt; 3))
  mask2 = img.mask().reduce(ee.Reducer.min())
  return img.updateMask(cloud.Not()).updateMask(mask2)  </code></pre>
<p>Now you will filter and reduce the entire Landsat-8 dataset, considering the following:</p>
<ol style="list-style-type: decimal">
<li><p>Select just bands <strong>[R, G, B, NIR]</strong>.</p></li>
<li><p>Filter considering the cloud pixel percentage by scene (&lt; 20%).</p></li>
<li><p>Filter considering a date (we just selecting 1 year)</p></li>
<li><p>Apply <strong>mask2cloud</strong> to each image.</p></li>
<li><p>Get the median of the ImageCollection.</p></li>
<li><p>Clip the image considering the study area.</p></li>
</ol>
<p><strong>NOTE:</strong> To apply a function on all the elements of specified <strong><code>ImageCollection</code></strong> or <strong><code>FeatureCollection</code></strong>, you can use the <strong><code>map()</code></strong> function.</p>
<pre class="python"><code># 4.4.3 Prepare the satellite image (Landsat-8)
RGB_bands = [&#39;B3&#39;,&#39;B2&#39;,&#39;B1&#39;] #RGB
NDVI_bands = [&#39;B4&#39;,&#39;B3&#39;] #NIR

l5 = ee.ImageCollection(&quot;LANDSAT/LT05/C01/T1_SR&quot;)\
               .filterBounds(db_crop)\
               .filterDate(&#39;2005-01-01&#39;, &#39;2006-12-31&#39;)\
               .filter(ee.Filter.lt(&#39;CLOUD_COVER&#39;, 20))\
               .map(maskS2clouds)\
               .median()\
               .multiply(0.0001)

l5_ndvi = l5.normalizedDifference(NDVI_bands).rename([&#39;NDVI&#39;])
l5_rgb = l5.select(RGB_bands).rename([&#39;R&#39;,&#39;G&#39;,&#39;B&#39;]) 
l5 = l5_rgb.addBands(l8_ndvi).addBands(glc30)</code></pre>
<pre class="python"><code>from collections import OrderedDict
# Create a visualization with folium
visParams_l5 = {    
  &#39;bands&#39;: [&#39;R&#39;, &#39;G&#39;, &#39;B&#39;],
  &#39;min&#39;: 0,
  &#39;max&#39;: 0.5,
  &#39;gamma&#39;: 1.4,
}

l5Mapid = l5.getMapId(visParams_l5)
dicc[&#39;Landsat5&#39;] = l8Mapid

# Changing the order of the dictionary
key_order = [&#39;Landsat5&#39;,&#39;glc30&#39;,&#39;CamanaValley&#39;,&#39;train&#39;,&#39;test&#39;]
dicc = OrderedDict((k, dicc[k]) for k in key_order)

Mapdisplay(center,dicc,zoom_start=8)</code></pre>
<center>
<img src="https://raw.githubusercontent.com/csaybar/csaybar.github.io/master/img/unet_04.png">
</center>
<p>The key to success for integrating GEE with a CNN pipeline is the <strong>ee.Image.neighborhoodToArray</strong> function. It turns the neighborhood of each pixel in a scalar image into a 2D array (see image below). Axes 0 and 1 of the output array correspond to Y and X axes of the image, respectively. The output image will have as many bands as the input; each output band has the same mask as the corresponding input band. The footprint and metadata of the input image are preserved.</p>
<center>
<img src = "https://raw.githubusercontent.com/csaybar/csaybar.github.io/master/img/neighborhoodToArray.png">
</center>
<p>There are several restrictions about the max number of features that a <a href="https://developers.google.com/earth-engine/exporting#exporting-tables-and-vector-data">table can save</a> (~ 10 M). For this reason, the <strong>saveCNN_batch</strong> function is created as below:</p>
<pre class="python"><code>import numpy as np
import time

def saveCNN_batch(image, point,kernel_size,scale,FilePrefix, selectors,folder, bucket=&#39;bag_csaybar&#39;):
  &quot;&quot;&quot;
    Export a dataset for semantic segmentation by batches
  
  Params:
  ------
    - image : ee.Image to get pixels from; must be scalar-valued.
    - point : Points to sample over.
    - kernel_size : The kernel specifying the shape of the neighborhood. Only fixed, square and rectangle kernels are supported.
      Weights are ignored; only the shape of the kernel is used.
    - scale : A nominal scale in meters of the projection to work in.
    - FilePrefix : Cloud Storage object name prefix for the export.
    - selector : Specified the properties to save.
    - bucket : The name of a Cloud Storage bucket for the export.  
  &quot;&quot;&quot;
  print(&#39;Found Cloud Storage bucket.&#39; if tf.io.gfile.exists(&#39;gs://&#39; + bucket) 
    else &#39;Output Cloud Storage bucket does not exist.&#39;)
  
  # Download the points (Server -&gt; Client)
  nbands = len(selectors)
  points = train_agriculture.geometry().getInfo()[&#39;coordinates&#39;]    
  nfeatures = kernel_size*kernel_size*nbands*len(points) #estimate the totals # of features
     
  image_neighborhood = image.neighborhoodToArray(ee.Kernel.rectangle(kernel_size, kernel_size, &#39;pixels&#39;))
  filenames = []
  
  #Threshold considering the max number of features permitted to export.
  if nfeatures &gt; 3e6:
    nparts = int(np.ceil(nfeatures/3e6))
    print(&#39;Dataset too long, splitting it into &#39;+ str(nparts),&#39;equal parts.&#39;)
    
    nppoints = np.array(points)
    np.random.shuffle(nppoints)
    
    count_batch = 1  # Batch counter 
    
    for batch_arr in np.array_split(nppoints,nparts):
      
      fcp = ee.FeatureCollection([
          ee.Feature(ee.Geometry.Point(p),{&#39;class&#39;:&#39;NA&#39;}) 
          for p in batch_arr.tolist() 
      ])
      
      # Agriculture dataset (fcp-points) collocation to each L5 grid cell value.
      train_db = image_neighborhood.sampleRegions(collection=fcp, scale=scale)
      filename = &#39;%s/%s-%04d_&#39; % (folder,FilePrefix,count_batch)
      
      # Create the tasks for passing of GEE to Google storage
      print(&#39;sending the task #%04d&#39;%count_batch)
      Task = ee.batch.Export.table.toCloudStorage(
        collection=train_db,        
        selectors=selectors,          
        description=&#39;Export batch &#39;+str(count_batch),
        fileNamePrefix=filename,
        bucket=bucket,  
        fileFormat=&#39;TFRecord&#39;)
      
      Task.start()
      filenames.append(filename)
      count_batch+=1
      
      while Task.active():
        print(&#39;Polling for task (id: {}).&#39;.format(Task.id))
        time.sleep(3)
        
    return filenames
  
  else:    
    train_db = image_neighborhood.sampleRegions(collection=points, scale=scale)         
    Task = ee.batch.Export.table.toCloudStorage(
      collection=train_db,
      selectors=selectors,
      description=&#39;Training Export&#39;,
      fileNamePrefix=FilePrefix,
      bucket=bucket,  
      fileFormat=&#39;TFRecord&#39;)
    Task.start()
    
    while Task.active():
      print(&#39;Polling for task (id: {}).&#39;.format(Task.id))
      time.sleep(3)
    
    return FilePrefix</code></pre>
<p>Unfortunately, you cannot use Tensorflow directly in Earth Engine. To overcome this situation,
the function <strong><code>saveCNN_batch</code></strong> use <strong>Google Cloud Storage Bucket (GCS, you could use Google Drive instead too)</strong> to save the dataset, since both GEE and Tensorflow can access to it. For more details about how to export data in GEE see the next <a href="10_Export.ipynb">link</a> or into the <a href="https://developers.google.com/earth-engine/exporting">Official Exporting data guide</a>.</p>
<pre class="python"><code>selectors = [&#39;R&#39;,&#39;G&#39;,&#39;B&#39;,&#39;NDVI&#39;,&#39;target&#39;]
train_filenames = saveCNN_batch(l8,train_agriculture,128,30,&#39;trainUNET&#39;, selectors,folder =&#39;unet&#39;, bucket=&#39;csaybar&#39;)
test_filenames = saveCNN_batch(l8,test_agriculture,128,30,&#39;testUNET&#39;, selectors,folder =&#39;unet&#39;, bucket=&#39;csaybar&#39;)</code></pre>
</div>
<div id="creating-a-tf.data.dataset-from-a-tfrecord-file" class="section level3">
<h3>4.5. Creating a tf.data.Dataset from a TFRecord file</h3>
<p>Read data from the TFRecord file into a tf.data.Dataset. Pre-process the dataset to get it into a suitable format for input to the DNN model.
For getting more details about <code>tf.data.Dataset</code>see the next <a href="https://www.tensorflow.org/guide/premade_estimators#create_input_functions">TFdoc</a>.</p>
<pre class="python"><code># Fullname train/test db
folder = &#39;unet&#39;
bucket = &#39;bag_csaybar&#39;

filesList = !gsutil ls &#39;gs://&#39;{bucket}&#39;/&#39;{folder}

trainFilePrefix = &#39;trainUNET&#39;
trainFilePath = [s for s in filesList if trainFilePrefix in s]


testFilePrefix = &#39;testUNET&#39;
testFilePath = [s for s in filesList if testFilePrefix in s]</code></pre>
<pre class="python"><code>def input_fn(fileNames, numEpochs=None, shuffle=True, batchSize=16, side = 257):
  # Read `TFRecordDatasets` 
  dataset = tf.data.TFRecordDataset(fileNames, compression_type=&#39;GZIP&#39;)

  # Names of the features 
  feature_columns = {
    &#39;R&#39;: tf.io.FixedLenFeature([side, side], dtype=tf.float32),  
    &#39;G&#39;: tf.io.FixedLenFeature([side, side], dtype=tf.float32),  
    &#39;B&#39;: tf.io.FixedLenFeature([side, side], dtype=tf.float32),    
    &#39;NDVI&#39;: tf.io.FixedLenFeature([side, side], dtype=tf.float32),    
    &#39;target&#39;: tf.io.FixedLenFeature([side, side], dtype=tf.float32)
  }
  
  # Make a parsing function
  def parse(example_proto):
    parsed_features = tf.io.parse_single_example(example_proto, feature_columns)   
    # passing of 257x257 to 256x256
    parsed_features = {key:value[1:side,1:side] for key,value in parsed_features.items()} 
    # Separate the class labels from the training features
    labels = parsed_features.pop(&#39;target&#39;)
    return parsed_features, tf.cast(labels, tf.int32)
  
  # Passing of FeatureColumns to a 4D tensor
  def stack_images(features,label):         
    nfeat = tf.transpose(tf.squeeze(tf.stack(list(features.values()))))
    nlabel = (tf.transpose(label))[:,:,tf.newaxis]
    return nfeat, nlabel
  
  dataset = dataset.map(parse, num_parallel_calls=4)
  dataset = dataset.map(stack_images, num_parallel_calls=4)
  
  if shuffle:
    dataset = dataset.shuffle(buffer_size = batchSize * 10)
  dataset = dataset.batch(batchSize)
  dataset = dataset.repeat(numEpochs)
  
  return dataset</code></pre>
<pre class="python"><code>train_dba = input_fn(trainFilePath,100,True,3)
test_dba = input_fn(testFilePath, numEpochs=1, batchSize=1, shuffle=False)</code></pre>
</div>
<div id="visualize" class="section level3">
<h3>4.6 Visualize</h3>
<p>Let’s take a look at some of the patches in our dataset.</p>
<pre class="python"><code>import matplotlib.pyplot as plt
import numpy as np

display_num = 5
plt.figure(figsize=(14, 21))

c=0
for i in range(1, display_num):
  for x in test_dba.take(i):
    x  
  tensor = tf.squeeze(x[0]).numpy()[:,:,[3,1,0]]
  target = tf.squeeze(x[1])

  #print(target.sum())  
  plt.subplot(display_num, 2, c + 1)
  plt.imshow(tensor)
  plt.title(&quot;RGB LANDSAT5&quot;)
  
  plt.subplot(display_num, 2, c + 2)
  plt.imshow(target)
  plt.title(&quot;Crop Area&quot;)
  c+=2 
plt.show()</code></pre>
<center>
<img src="https://raw.githubusercontent.com/csaybar/csaybar.github.io/master/img/unet_05.png">
</center>
</div>
<div id="set-up" class="section level3">
<h3>4.7 Set up</h3>
<p>Let’s begin by setting up some constant parameters.</p>
<pre class="python"><code>IMG_SHAPE  = (256, 256, 4)
EPOCHS = 10</code></pre>
</div>
<div id="creating-a-u-net-model-with-keras" class="section level3">
<h3>4.8. Creating a U-NET model with keras</h3>
<p>(For very complete guide about convolution arithmetic see <a href="https://arxiv.org/pdf/1603.07285.pdf?">this paper</a>).</p>
<p>Here you will create a convolutional neural network model with:
- 5 encoder layers.
- 5 decoder layer.
- 1 output layer.</p>
<p>The <strong>encoder layer</strong> is composed of a linear stack of <code>Conv</code>, <code>BatchNorm</code>, and <code>Relu</code> operations followed by a <code>MaxPool</code>. Each <code>MaxPool</code> will reduce the spatial resolution of our feature map by a factor of 2. We keep track of the outputs of each block as we feed these high-resolution feature maps with the decoder portion.</p>
<p>The <strong>decoder layer</strong> is comprised of <code>UpSampling2D</code>, <code>Conv</code>, <code>BatchNorm</code>, and <code>Relu</code>. Note that we concatenate the feature map of the same size on the decoder side. Finally, we add a final Conv operation that performs a convolution along the channels for each individual pixel (kernel size of (1, 1)) that outputs our final segmentation mask in grayscale.</p>
<p>Additionally, Early Stopping, Tensorboard, and best model callback were added. A callback is a set of functions to be applied at given stages of the training procedure. You can found more details <a href="https://keras.io/callbacks/">here</a>.</p>
<pre class="python"><code>from tensorflow.keras import layers

def conv_block(input_tensor, num_filters):
  encoder = layers.Conv2D(num_filters, (3, 3), padding=&#39;same&#39;)(input_tensor)
  encoder = layers.BatchNormalization()(encoder)
  encoder = layers.Activation(&#39;relu&#39;)(encoder)
  encoder = layers.Conv2D(num_filters, (3, 3), padding=&#39;same&#39;)(encoder)
  encoder = layers.BatchNormalization()(encoder)
  encoder = layers.Activation(&#39;relu&#39;)(encoder)
  return encoder

def encoder_block(input_tensor, num_filters):
  encoder = conv_block(input_tensor, num_filters)
  encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)
  
  return encoder_pool, encoder

def decoder_block(input_tensor, concat_tensor, num_filters):
  decoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding=&#39;same&#39;)(input_tensor)
  decoder = layers.concatenate([concat_tensor, decoder], axis=-1)
  decoder = layers.BatchNormalization()(decoder)
  decoder = layers.Activation(&#39;relu&#39;)(decoder)
  decoder = layers.Conv2D(num_filters, (3, 3), padding=&#39;same&#39;)(decoder)
  decoder = layers.BatchNormalization()(decoder)
  decoder = layers.Activation(&#39;relu&#39;)(decoder)
  decoder = layers.Conv2D(num_filters, (3, 3), padding=&#39;same&#39;)(decoder)
  decoder = layers.BatchNormalization()(decoder)
  decoder = layers.Activation(&#39;relu&#39;)(decoder)
  return decoder

inputs = layers.Input(shape=IMG_SHAPE)
# 256
encoder0_pool, encoder0 = encoder_block(inputs, 32)
# 128
encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)
# 64
encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)
# 32
encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)
# 16
encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)
# 8
center = conv_block(encoder4_pool, 1024)
# center
decoder4 = decoder_block(center, encoder4, 512)
# 16
decoder3 = decoder_block(decoder4, encoder3, 256)
# 32
decoder2 = decoder_block(decoder3, encoder2, 128)
# 64
decoder1 = decoder_block(decoder2, encoder1, 64)
# 128
decoder0 = decoder_block(decoder1, encoder0, 32)
# 256

outputs = layers.Conv2D(1, (1, 1), activation=&#39;sigmoid&#39;)(decoder0)</code></pre>
</div>
<div id="define-your-model" class="section level3">
<h3>4.9 Define your model</h3>
<p>Using functional API, you must define your model by specifying the inputs and outputs associated with the model.</p>
<pre class="python"><code>from tensorflow.keras import models
model = models.Model(inputs=[inputs], outputs=[outputs])
model.summary()</code></pre>
<center>
<img src="https://raw.githubusercontent.com/csaybar/csaybar.github.io/master/img/unet_06.png">
</center>
</div>
<div id="defining-custom-metrics-and-loss-functions" class="section level3">
<h3>4.10. Defining custom metrics and loss functions</h3>
<p>Defining loss and metric functions are simple with Keras. Simply define a function that takes both the True labels for a given example and the Predicted labels for the same given example.</p>
<p><strong>Dice loss</strong> is a metric that measures overlap. More info on optimizing for Dice coefficient (our dice loss) can be found in the <a href="http://campar.in.tum.de/pub/milletari2016Vnet/milletari2016Vnet.pdf">paper</a>, where it was introduced. We use dice loss here because it <strong>performs better at class imbalanced problems by design</strong>. Using cross entropy is more of a proxy which is easier to maximize. Instead, we maximize our objective directly.</p>
<pre class="python"><code>from tensorflow.keras import losses

def dice_coeff(y_true, y_pred):
    smooth = 1.
    # Flatten
    y_true_f = tf.reshape(y_true, [-1])
    y_pred_f = tf.reshape(y_pred, [-1])
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)
    return score

def dice_loss(y_true, y_pred):
    loss = 1 - dice_coeff(y_true, y_pred)
    return loss

def bce_dice_loss(y_true, y_pred):
  loss = losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)
  return loss</code></pre>
</div>
<div id="compiling-the-model" class="section level3">
<h3>4.11 Compiling the model</h3>
<p>We use our custom loss function to minimize. In addition, we specify what metrics we want to keep track of as we train. Note that metrics are not actually used during the training process to tune the parameters, but are instead used to measure performance of the training process.</p>
<pre class="python"><code>from tensorflow.keras.utils import plot_model
model.compile(optimizer=&#39;adam&#39;, loss=bce_dice_loss, metrics=[dice_loss])
plot_model(model)</code></pre>
</div>
<div id="training-the-model-optional" class="section level3">
<h3>4.12 Training the model (OPTIONAL)</h3>
<p>Training your model with <code>tf.data</code> involves simply providing the model’s <code>fit</code> function with your training/validation dataset, the number of steps, and epochs.</p>
<p>We also include a Model callback, <a href="https://keras.io/callbacks/#modelcheckpoint"><code>ModelCheckpoint</code></a> that will save the model to disk after each epoch. We configure it such that it only saves our highest performing model. Note that saving the model capture more than just the weights of the model: by default, it saves the model architecture, weights, as well as information about the training process such as the state of the optimizer, etc.</p>
<pre class="python"><code>from tensorflow import keras
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import os
import datetime

# Callbacks time
logdir = os.path.join(&quot;logs&quot;, datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
es = EarlyStopping(monitor=&#39;val_loss&#39;, patience=10)
mcp = ModelCheckpoint(filepath=&#39;best_model.h5&#39;, monitor=&#39;val_loss&#39;, save_best_only=True)</code></pre>
<pre class="python"><code># Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount(&#39;/content/drive&#39;)</code></pre>
<pre class="python"><code>N_train = 550
batch_size = 3

# Train the model I just do it for 15 minutes
history = model.fit(train_dba,
                    steps_per_epoch= int(np.ceil(N_train / float(batch_size))),
                    epochs=EPOCHS,
                    validation_data=test_dba,
                    callbacks=[tensorboard_callback,es,mcp])</code></pre>
<pre class="python"><code>%tensorboard --logdir logs
#!kill 607</code></pre>
<pre class="python"><code>import urllib
url = &#39;https://storage.googleapis.com/bag_csaybar/unet/best_model.h5&#39;
urllib.request.urlretrieve(url, &#39;best_model.h5&#39;)</code></pre>
</div>
<div id="loading-a-pretrained-model-3-epochs" class="section level3">
<h3>4.13 Loading a Pretrained model (3 epochs)</h3>
<pre class="python"><code>import urllib
url = &#39;https://storage.googleapis.com/bag_csaybar/unet/best_model.h5&#39;
urllib.request.urlretrieve(url, &#39;best_model.h5&#39;)</code></pre>
<pre class="python"><code>model.load_weights(&quot;best_model.h5&quot;)
model.evaluate(x = test_dba)</code></pre>
</div>
<div id="prediction" class="section level3">
<h3>4.14. Prediction</h3>
<p>You will prepare the L5 imagery, likewise, you made it for the train/test dataset.</p>
<pre class="python"><code>l5 = ee.ImageCollection(&quot;LANDSAT/LT05/C01/T1_SR&quot;)\
               .filterBounds(Camana_valley)\
               .filterDate(&#39;2005-01-01&#39;, &#39;2006-12-31&#39;)\
               .filter(ee.Filter.lt(&#39;CLOUD_COVER&#39;, 20))\
               .map(maskS2clouds)\
               .median()\
               .multiply(0.0001)

l5_ndvi = l5.normalizedDifference(NDVI_bands).rename([&#39;NDVI&#39;])
l5_rgb = l5.select(RGB_bands).rename([&#39;R&#39;,&#39;G&#39;,&#39;B&#39;]) 
l5 = l5_rgb.addBands(l5_ndvi)</code></pre>
<pre class="python"><code>from collections import OrderedDict

# Vizualize the dataset
l5id = l5.clip(Camana_valley.buffer(2500)).getMapId({&#39;max&#39;:0.6,&#39;min&#39;:0})
center = Camana_valley.centroid().getInfo()[&#39;coordinates&#39;]
center.reverse()
Mapdisplay(center,{&#39;l5id&#39;:l5id},zoom_start=11)</code></pre>
<center>
<img src="https://raw.githubusercontent.com/csaybar/csaybar.github.io/master/img/unet_08.png">
</center>
<p>For export the results to the Google Cloud Storage, it’s preferred defines the following <code>formatOptions</code> parameters to save memory:</p>
<ul>
<li><p><strong>patchDimensions</strong>: Patch dimensions tiled over the export area, covering every pixel in the bounding box exactly once (except when the patch dimensions do not evenly divide the bounding box in which case the lower and right sides are trimmed).</p></li>
<li><p><strong>compressed</strong>: If true, compresses the .tfrecord files with gzip and appends the “.gz” suffix</p></li>
</ul>
<p>See all the paramerters <a href="https://developers.google.com/earth-engine/exporting#configuration-parameters">here</a>.</p>
<pre class="python"><code>outputBucket = &#39;bag_csaybar&#39;
imageFilePrefix = &#39;unet/Predict_CamanaValleyCrop&#39;

# Specify patch and file dimensions.
imageExportFormatOptions = {
  &#39;patchDimensions&#39;: [256, 256],
  &#39;compressed&#39;: True
}

# Setup the task.
imageTask = ee.batch.Export.image.toCloudStorage(
  image=l5,
  description=&#39;Image Export&#39;,
  fileNamePrefix=imageFilePrefix,
  bucket=outputBucket,
  scale=30,
  fileFormat=&#39;TFRecord&#39;,
  region=Camana_valley.buffer(2500).getInfo()[&#39;coordinates&#39;],
  formatOptions=imageExportFormatOptions,
)

imageTask.start()</code></pre>
<pre class="python"><code>import time 
while imageTask.active():
  print(&#39;Polling for task (id: {}).&#39;.format(imageTask.id))
  time.sleep(5)</code></pre>
<p>Now it’s time to classify the image that was exported from GEE to GCS using Tensorflow. If the exported image is large (it not your case), it will be split into multiple TFRecord files in its destination folder. There will also be a JSON sidecar file called <strong>“the mixer”</strong> that describes the format and georeferencing of the image. Here we will find the image files and the mixer file, getting some info out of the mixer that will be useful during model inference.</p>
<pre class="python"><code>filesList = !gsutil ls &#39;gs://&#39;{outputBucket}&#39;/unet/&#39;
exportFilesList = [s for s in filesList if imageFilePrefix in s]

# Get the list of image files and the JSON mixer file.
imageFilesList = []
jsonFile = None
for f in exportFilesList:
  if f.endswith(&#39;.tfrecord.gz&#39;):
    imageFilesList.append(f)
  elif f.endswith(&#39;.json&#39;):
    jsonFile = f

# Make sure the files are in the right order.
print(jsonFile)</code></pre>
<p>The mixer contains metadata and georeferencing information for the exported patches, each of which is in a different file. Read the mixer to get some information needed for prediction.</p>
<pre class="python"><code>import json
from pprint import pprint 

# Load the contents of the mixer file to a JSON object.
jsonText = !gsutil cat {jsonFile}
# Get a single string w/ newlines from the IPython.utils.text.SList
mixer = json.loads(jsonText.nlstr)
pprint(mixer)</code></pre>
<p>The next function is slightly different from the to the <code>input_fn</code> (see Section 4.5). Mainly, this is because the pixels are written into records as patches, we need to read the patches in as one big tensor (one patch for each band), then flatten them into lots of little tensors. Once the <code>predict_input_fn</code> is defined, that can handle the shape of the image data, all you need to do is feed it directly to the trained model to make predictions.</p>
<pre class="python"><code>def predict_input_fn(fileNames,side,bands):
  
  # Read `TFRecordDatasets` 
  dataset = tf.data.TFRecordDataset(fileNames, compression_type=&#39;GZIP&#39;)

  featuresDict = {x:tf.io.FixedLenFeature([side, side], dtype=tf.float32) for x in bands}
     
  # Make a parsing function
  def parse_image(example_proto):
    parsed_features = tf.io.parse_single_example(example_proto, featuresDict)
    return parsed_features
  
  def stack_images(features):         
    nfeat = tf.transpose(tf.squeeze(tf.stack(list(features.values()))))    
    return nfeat
 
  dataset = dataset.map(parse_image, num_parallel_calls=4)
  dataset = dataset.map(stack_images, num_parallel_calls=4)   
  dataset = dataset.batch(side*side)

  return dataset</code></pre>
<pre class="python"><code>predict_db = predict_input_fn(fileNames=imageFilesList,side=256,bands=[&#39;R&#39;, &#39;G&#39;, &#39;B&#39;, &#39;NDVI&#39;])
predictions = model.predict(predict_db)</code></pre>
<p>Now that there’s a <code>np.array</code> of probabilities in “predictions”, it’s time to write them back into a file. You will write directly from TensorFlow to a file in the output Cloud Storage bucket.</p>
<p>Iterate over the list and write the probabilities in patches. Specifically, we need to write the pixels into the file as patches in the same order they came out. The records are written as serialized tf.train.Example protos. This might take a while.</p>
<pre class="python"><code># Instantiate the writer.
PATCH_WIDTH , PATCH_HEIGHT = [256,256]
outputImageFile = &#39;gs://&#39; + outputBucket + &#39;/unet/CamanaValleyCrop.TFRecord&#39;
writer = tf.io.TFRecordWriter(outputImageFile)

# Every patch-worth of predictions we&#39;ll dump an example into the output
# file with a single feature that holds our predictions. Since our predictions
# are already in the order of the exported data, the patches we create here
# will also be in the right order.
curPatch = 1
for  prediction in predictions:
  patch = prediction.squeeze().T.flatten().tolist()
  
  if (len(patch) == PATCH_WIDTH * PATCH_HEIGHT):
    print(&#39;Done with patch &#39; + str(curPatch) + &#39;...&#39;)    
    # Create an example
    example = tf.train.Example(
      features=tf.train.Features(
        feature={
          &#39;crop_prob&#39;: tf.train.Feature(
              float_list=tf.train.FloatList(
                  value=patch))
        }
      )
    )
    
    writer.write(example.SerializeToString())    
    curPatch += 1 

writer.close()</code></pre>
</div>
<div id="upload-the-classifications-to-an-earth-engine-asset" class="section level3">
<h3>4.15 Upload the classifications to an Earth Engine asset</h3>
<p>At this stage, there should be a predictions TFRecord file sitting in the output Cloud Storage bucket. Use the gsutil command to verify that the predictions image (and associated mixer JSON) exist and have non-zero size.</p>
<pre class="python"><code>!gsutil ls -l {outputImageFile}</code></pre>
<p>Upload the image to Earth Engine directly from the Cloud Storage bucket with the <a href="https://developers.google.com/earth-engine/command_line#upload">earthengine command</a>. Provide both the image TFRecord file and the JSON file as arguments to earthengine upload.</p>
<pre class="python"><code># REPLACE WITH YOUR USERNAME:
USER_NAME = &#39;csaybar&#39;
outputAssetID = &#39;users/&#39; + USER_NAME + &#39;/CamanaCrop_UNET&#39;
print(&#39;Writing to &#39; + outputAssetID)</code></pre>
<pre class="python"><code># Start the upload. It step might take a while.
!earthengine upload image --asset_id={outputAssetID} {outputImageFile} {jsonFile}</code></pre>
<p>Display the Results using Folium!</p>
<pre class="python"><code>ProbsImage = ee.Image(outputAssetID)
predictionsImage = ee.Image(outputAssetID).gte(0.500)
dicc = {&#39;CropProbability&#39;:ProbsImage.getMapId({&#39;min&#39;:0.49,&#39;max&#39;:0.498}),
        &#39;Crop&#39;:predictionsImage.getMapId()}

center = Camana_valley.centroid().getInfo()[&#39;coordinates&#39;]
center.reverse()

Mapdisplay(center=center,dicc=dicc,zoom_start=13)</code></pre>
<center>
<img src="https://raw.githubusercontent.com/csaybar/csaybar.github.io/master/img/unet_09.png">
</center>
</div>
<div id="thats-all-for-this-time-the-next-post-is-about-sequential-models-and-earth-engine." class="section level3">
<h3>That’s all for this time!, the next post is about sequential models and Earth Engine.</h3>
</div>
</div>
