>>> obj, num= fast_view(example)
"""
object_describe, numeric_describe = None,None
cnt = Counter(df.dtypes)
if verbose:
print('number of features %s' % df.shape[1])
for typo, numb in cnt.items():
print('type_feature:% s n:%s' % (typo,numb))
features_name = df.columns
types = pd.Series([str(x) for x in df.dtypes.values])
# Object type analysis
if sum(types == 'object') is not 0:
object_col_n = types[types.isin(['object'])].index
object_features = df.iloc[:,object_col_n]
f_object_name = object_features.columns
# add NaN analysis -- object
obj_type_len = {x:(sum(object_features[x].isin([np.NaN]))>=1,
np.round(sum(object_features[x].isin([np.NaN]))/df.shape[0],3)
) for x in f_object_name}
resumen_pd_obj = pd.DataFrame(obj_type_len, index = ['NaN_exist?','%perc_NA'])
other_info_obj = df.describe(include=np.object)
object_describe = pd.concat([other_info_obj, resumen_pd_obj], sort = True)
# Numeric type analysis
if sum(types == 'int64') is not 0 or sum(types == 'float64') is not 0:
numeric_col_n = types[~types.isin(['object'])].index # index of numeric columns
numeric_features = df.iloc[:, numeric_col_n] # numeric columns
f_numeric_name = numeric_features.columns # numeric columns name
# 1. add NaN analysis -- numeric
NaN_dict = {x:(sum(numeric_features[x].isin([np.NaN]))>=1,
np.round(sum(numeric_features[x].isin([np.NaN]))/df.shape[0],3)
) for x in f_numeric_name}
NaN_pd = pd.DataFrame(NaN_dict, index = ['NaN_exist?','%perc_NA'])
# 2. add outlier
outlier_dict = {x:(np.sum(tukey_outlier(numeric_features[x])),
np.sum(zscore_outlier(numeric_features[x]))
) for x in f_numeric_name}
outlier_pd = pd.DataFrame(outlier_dict, index = ['tukey_outlier_1.5','zscore_outlier_7.5'])
# 3. add normality information
normality_dict = {x:(shapiro_ps(numeric_features[x]),
agostino_ps(numeric_features[x]),
numeric_features[x].skew(skipna = True),
kurtosis(numeric_features[x],nan_policy='omit')
) for x in f_numeric_name}
normality_pd = pd.DataFrame(normality_dict, index = ['shapiro_pvalue','DAgostino_pvalue','Skew','Kurtosis'])
# 4. Simple describe
other_info_num = df.describe()
numeric_describe = pd.concat([other_info_num, resumen_pd_num, outlier_pd, normality_pd], sort = True)
return object_describe, numeric_describe
from __future__ import division, absolute_import, print_function
__all__ = ["fast_view", "shapiro_ps","agostino_ps", "zscore_outlier", "tukey_outlier",]
from collections import Counter
import pandas as pd
import numpy as np
from scipy.stats import normaltest
from scipy.stats import shapiro
from scipy.stats import kurtosis
def fast_view(df, verbose = True):
"""
`fast_view` generate descriptive statistics that summarize:
- Central tendency
- Dispersion
- Shape
- `NaN` values
- Outliers
- Normality
`fast_view` automatically detect both numeric and object series
Parameters
----------
df : pd.DataFrame
verbose : enable showing shape information
Returns
-------
A tuple (x, y) that contains:
x -> summary statistics for object series
y -> summary statistics for numeric series
See Also
--------
pd.DataFrame.describe : A more general describe statistics.
Examples
--------
>>> from preml.utils import fast_view
>>> example = pd.DataFrame({'A':['a','c','a'],'B':[1,2,3]})
>>> obj, num= fast_view(example)
"""
object_describe, numeric_describe = None,None
cnt = Counter(df.dtypes)
if verbose:
print('number of features %s' % df.shape[1])
for typo, numb in cnt.items():
print('type_feature:% s n:%s' % (typo,numb))
features_name = df.columns
from __future__ import division, absolute_import, print_function
__all__ = ["fast_view", "shapiro_ps","agostino_ps", "zscore_outlier", "tukey_outlier",]
from collections import Counter
import pandas as pd
import numpy as np
from scipy.stats import normaltest
from scipy.stats import shapiro
from scipy.stats import kurtosis
def fast_view(df, verbose = True):
"""
`fast_view` generate descriptive statistics that summarize:
- Central tendency
- Dispersion
- Shape
- `NaN` values
- Outliers
- Normality
`fast_view` automatically detect both numeric and object series
Parameters
----------
df : pd.DataFrame
verbose : enable showing shape information
Returns
-------
A tuple (x, y) that contains:
x -> summary statistics for object series
y -> summary statistics for numeric series
See Also
--------
pd.DataFrame.describe : A more general describe statistics.
Examples
--------
>>> from preml.utils import fast_view
>>> example = pd.DataFrame({'A':['a','c','a'],'B':[1,2,3]})
>>> obj, num= fast_view(example)
"""
object_describe, numeric_describe = None,None
cnt = Counter(df.dtypes)
if verbose:
print('number of features %s' % df.shape[1])
for typo, numb in cnt.items():
print('type_feature:% s n:%s' % (typo,numb))
features_name = df.columns
types = pd.Series([str(x) for x in df.dtypes.values])
# Object type analysis
if sum(types == 'object') is not 0:
object_col_n = types[types.isin(['object'])].index
object_features = df.iloc[:,object_col_n]
f_object_name = object_features.columns
# add NaN analysis -- object
obj_type_len = {x:(sum(object_features[x].isin([np.NaN]))>=1,
np.round(sum(object_features[x].isin([np.NaN]))/df.shape[0],3)
) for x in f_object_name}
resumen_pd_obj = pd.DataFrame(obj_type_len, index = ['NaN_exist?','%perc_NA'])
other_info_obj = df.describe(include=np.object)
object_describe = pd.concat([other_info_obj, resumen_pd_obj], sort = True)
# Numeric type analysis
if sum(types == 'int64') is not 0 or sum(types == 'float64') is not 0:
numeric_col_n = types[~types.isin(['object'])].index # index of numeric columns
numeric_features = df.iloc[:, numeric_col_n] # numeric columns
f_numeric_name = numeric_features.columns # numeric columns name
# 1. add NaN analysis -- numeric
NaN_dict = {x:(sum(numeric_features[x].isin([np.NaN]))>=1,
np.round(sum(numeric_features[x].isin([np.NaN]))/df.shape[0],3)
) for x in f_numeric_name}
NaN_pd = pd.DataFrame(NaN_dict, index = ['NaN_exist?','%perc_NA'])
# 2. add outlier
outlier_dict = {x:(np.sum(tukey_outlier(numeric_features[x])),
np.sum(zscore_outlier(numeric_features[x]))
) for x in f_numeric_name}
outlier_pd = pd.DataFrame(outlier_dict, index = ['tukey_outlier_1.5','zscore_outlier_7.5'])
# 3. add normality information
normality_dict = {x:(shapiro_ps(numeric_features[x]),
agostino_ps(numeric_features[x]),
numeric_features[x].skew(skipna = True),
kurtosis(numeric_features[x],nan_policy='omit')
) for x in f_numeric_name}
normality_pd = pd.DataFrame(normality_dict, index = ['shapiro_pvalue','DAgostino_pvalue','Skew','Kurtosis'])
# 4. Simple describe
other_info_num = df.describe()
numeric_describe = pd.concat([other_info_num, resumen_pd_num, outlier_pd, normality_pd], sort = True)
return object_describe, numeric_describe
serve_site()
build_site()
library(blogdown)
#blogdown::install_hugo()
serve_site()
build_site()
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
py$df
library(reticulate)
py$df
py$df
library(dplyr)
library(reticulate)
library(dplyr)
py$df %>% as_tibble()
library(blogdown)
blogdown::build_site()
blogdown::serve_site()
knitr::opts_chunk$set(echo = TRUE,eval = FALSE)
library(reticulate)
library(mapview)
mapview(c(xmin,ymin,xmax,ymax))
xmin =-72.778645
ymin =-16.621663
xmax =-72.66865
ymax = -16.57553
library(mapview)
mapview(c(xmin,ymin,xmax,ymax))
matrix(c(0,0,10,0,10,10,0,10,0,0),ncol=2, byrow=TRUE)
polygon_m = matrix(c(xmin,ymin,xmin,ymax,xmax,ymax,xmax,ymin,xmin,ymin),ncol=2, byrow=TRUE)
polygon_m
camana = st_polygon(pts)
library(sf)
polygon_m = matrix(c(xmin,ymin,xmin,ymax,xmax,ymax,xmax,ymin,xmin,ymin),ncol=2, byrow=TRUE)
camana = st_polygon(pts)
library(sf)
polygon_m = matrix(c(xmin,ymin,xmin,ymax,xmax,ymax,xmax,ymin,xmin,ymin),ncol=2, byrow=TRUE)
camana = st_polygon(polygon_m)
camana = st_polygon(list(polygon_m))
mapview(camana)
camana
st_sf(camana)
st_sf(camana,id=1)
st_sfc(camana)
camana = st_sfc(st_polygon(list(polygon_m)))
mapview(camana)
st_crs(camana)
st_crs(camana) = 4326
mapview(camana)
library(blogdown)
blogdown::build_site()
blogdown::serve_site()
knitr::opts_chunk$set(echo = TRUE,eval = FALSE)
library(dplyr)
library(dplyr)
read.csv('../../data/px.csv')
dataset = read.csv('../../data/px.csv')
dataset
dataset[2:4]
dataset = read.csv('../../data/px.csv')[2:4]
dataset
library(sp)
coordinates(dataset) = ~x+y
dataset
coordinates(dataset) = ~Longitude+Latitude
dataset
st_sf(dataset)
st_sfc(dataset)
dataset
st_as_sf(dataset)
st_as_sfc(dataset)
library(sf)
st_as_sfc(dataset)
dt = st_as_sfc(dataset)
st_crs(dt) = 4326
dt
mapview(dt,popup='Camana Valley')
library(mapview)
mapview(dt,popup='Camana Valley')
dt
dataset = read.csv('../../data/px.csv')[2:4]
coordinates(dataset) = ~Longitude+Latitude
dataset
dt = st_as_sfc(dataset)
dt
dt = st_as_sf(dataset)
dt
mapview(dt,popup='Camana Valley'color = "class")
mapview(dt,popup='Camana Valley',color = "class")
mapview(dt,color = "class")
mapview(dt,zcol = "class")
library(blogdown)
blogdown::build_site()
blogdown::serve_site()
blogdown::serve_site()
library(blogdown)
blogdown::build_site()
blogdown::serve_site()
install.packages("STARTS")
library(blogdown)
blogdown::build_site()
blogdown::serve_site()
library(blogdown)
blogdown::serve_site()
blogdown::build_site()
library(raster)
piscop = brick('/home/aybarpc01/Downloads/PISCOpm21.nc')
piscop
piscop
plot(piscop)
plot(piscop[[1]])
seq(as.Date("1981-01-01"),as.Date("2016-12-31"))
seq(as.Date("1981-01-01"),as.Date("2016-12-31"),'month')
serie = seq(as.Date("1981-01-01"),as.Date("2016-12-31"),'month')
serie
paste0('PISCO',serie)
paste0('PISCO_',serie)
library(raster)
piscop = brick('/home/aybarpc01/Downloads/PISCOpm21.nc')
serie = seq(as.Date("1981-01-01"),as.Date("2016-12-31"),'month')
my_name = paste0('PISCO_',serie)
library(raster)
piscop = brick('/home/aybarpc01/Downloads/PISCOpm21.nc')
serie = seq(as.Date("1981-01-01"),as.Date("2016-12-31"),'month')
my_name = paste0('PISCO_',serie)
piscopx = piscop[[1]]*100
piscopx
writeRaster(piscopx,my_name[1],dataType='INT2S')
my_name
setwd("/home/aybarpc01/Downloads/PISCOp")
piscopx = piscop[[1]]*100
writeRaster(piscopx,my_name[1],dataType='INT2S')
my_name = paste0('PISCO_',serie,'.tif')
setwd("/home/aybarpc01/Downloads/PISCOp")
piscopx = piscop[[1]]*100
writeRaster(piscopx,my_name[1],dataType='INT2S')
piscop
for (x in 1:432) {
piscopx = piscop[[x]]*100
writeRaster(piscopx,my_name[x],dataType='INT2S')
}
library(rts)
serie
serie
piscop
serie
piscopm <- rts(piscop, serie)
piscopm
for (x in 1:432) {
piscopx = piscop[[x]]*100
writeRaster(piscopx,my_name[x],dataType='INT2S')
}
SILT = raster('/home/aybarpc01/Downloads/RUSLE/SILT.tif')
SILT
dataType(SILT)
SILT = raster('/home/aybarpc01/Downloads/RUSLE/ORGCAR.tif')
dataType(SILT)
SILT
library(usethis)
devtools::install_github("r-lib/usethis")
path <- file.path("/home/aybarpc01/Desktop/", "mypkg")
path <- file.path("/home/aybarpc01/Desktop/", "PVts")
create_package(path)
library(usethis)
library(usethis)
library(usethis)
path <- file.path("/home/aybarpc01/Desktop/", "PVts")
create_package(path)
use_pkgdown()
path <- file.path("/home/aybarpc01/Desktop/", "PVts")
create_package(path)
library(blogdown)
blogdown::build_site()
library(blogdown)
#blogdown::install_hugo()
serve_site()
build_site()
library(blogdown)
#blogdown::install_hugo()
serve_site()
build_site()
source('~/.active-rstudio-document', echo=TRUE)
library(blogdown)
#blogdown::install_hugo()
serve_site()
blogdown::build_site()
library(blogdown)
install.packages("smoothr")
blogdown::build_site()
install.packages("lwgeom")
blogdown::build_site()
install.packages("rasterVis")
blogdown::build_site()
install.packages("kableExtra")
library(blogdown)
blogdown::build_site()
install.packages("furrr")
library(blogdown)
blogdown::build_site()
blogdown::install_hugo("0.55.5")
#blogdown::install_hugo("0.55.5")
blogdown::build_site()
blogdown::serve_site()
blogdown::serve_site()
load("/tmp/RtmpnZmT6O/rgee_session_by_selenium.Rdata")
session
library(reticulate)
session
session
session
session
rgee:::ee_get_upload_url(session)
ee_Initialize
library(rgee)
ee_Initialize()
ee_Initialize(user = 'aybar1994@gmail.com')
rgee:::ee_get_upload_url(session)
session
rgee:::ee_get_upload_url(session)
load("/tmp/RtmpnZmT6O/rgee_session_by_selenium.Rdata")
library(reticulate)
library(rgee)
load("/tmp/RtmpnZmT6O/rgee_session_by_selenium.Rdata")
ee_Initialize(user = 'aybar1994@gmail.com')
rgee:::ee_get_upload_url(session)
library(blogdown)
#blogdown::install_hugo("0.55.5")
blogdown::build_site()
blogdown::serve_site()
library(blogdown)
#blogdown::install_hugo("0.55.5")
blogdown::build_site()
blogdown::serve_site()
# grDevices::dev.size("px")
tmap_animation(tm = m1, width = 699*3,height = 555*3,delay = 100)
library(gdalcubes)
library(stars)
library(rgee)
ee_Initialize(drive = TRUE)
ocona <- ee$Geometry$Point(c(-73.10783, -16.43148))$buffer(1000)
ee_search_dataset() %>%
ee_search_title("sentinel") %>%
ee_search_title("MSI") %>%
ee_search_display()
s2 <- ee$ImageCollection("COPERNICUS/S2_SR")
getQABits <- function(image, qa) {
# Convert decimal (character) to decimal (little endian)
qa <- sum(2^(which(rev(unlist(strsplit(as.character(qa), "")) == 1))-1))
# Return a single band image of the extracted QA bits, giving the qa value.
image$bitwiseAnd(qa)$lt(1)
}
s2_clean <- function(img) {
# Estimate the NDVI from B8 and B4
ndvi <- img$normalizedDifference(c("B8", "B4"))
# Extract quality band
ndvi_qa <- img$select("QA60")
# Select pixels to mask
quality_mask <- getQABits(ndvi_qa, "110000000000")
# Mask pixels with value zero.
ndvi$updateMask(quality_mask)$copyProperties(
img,
c('system:id', 'system:time_start','system:time_end')
)
}
s2_ocona <- s2$
filterBounds(ocona)$
filter(ee$Filter$lte("CLOUDY_PIXEL_PERCENTAGE", 20))$
filter(ee$Filter$date("2017-01-01", as.character(Sys.Date())))$
filter(ee$Filter$calendarRange(6, field = "month"))$
map(s2_clean)
nimages <- s2_ocona$size()$getInfo()
ic_date <- ee_get_date_ic(s2_ocona)
Map$centerObject(ocona,zoom = 8)
s2_img_list <- list()
for (index in seq_len(nimages)) {
py_index <- index - 1
s2_img <- ee$Image(s2_ocona$toList(1, py_index)$get(0))
s2_img_list[[index]] <- Map$addLayer(
eeObject = s2_img,
visParams = list(min = -0.1, max = 0.8, palette = cpt("grass_ndvi", 10)),
name = ic_date$id[index]
)
}
Reduce('+', s2_img_list)
s2_ic_local <- ee_imagecollection_to_local(
ic = s2_ocona,
scale = 10,
region = ocona,
via = 'drive'
)
s2_stars <- s2_ic_local %>%
read_stars %>%
merge %>%
st_set_dimensions(names = c("x", "y", "NDVI")) %>%
`names<-`("NDVI")
s2_stars %>%
st_get_dimension_values(3) %>%
substr(
start = 2,
stop = 9
) %>%
as.Date(format="%Y%m%d") %>%
as.character() %>%
sprintf("Ocoña Valley, Arequipa, Peru: %s", .) ->
s2_new_names
m1 <- tm_shape(s2_stars) +
tm_raster(
palette = cpt("grass_ndvi", 20),
n = 20,
title = "NDVI",
style = "fisher") +
tmap_style(style = "natural") +
tm_facets(nrow = 1, ncol = 1) +
tm_layout(
frame.lwd = 2,
panel.label.bg.color = NA,
attr.outside = TRUE,
panel.show = FALSE,
legend.title.size = 1,
legend.title.fontface = 2,
legend.text.size = 0.7,
legend.frame = FALSE,
legend.outside = TRUE,
legend.position = c(0.20, 0.15),
legend.bg.color = "white",
legend.bg.alpha = 1,
main.title = sprintf("Ocoña Valley, Arequipa, Peru: %s", s2_new_names),
main.title.size = 1.2,
main.title.fontface = 2
)+
tm_credits(
text = "Source: Sentinel-2 MSI: MultiSpectral Instrument, Level-2A",
size = 1,
just = "right"
)
# grDevices::dev.size("px")
tmap_animation(tm = m1, width = 699*3,height = 555*3,delay = 100)
blogdown::serve_site()
